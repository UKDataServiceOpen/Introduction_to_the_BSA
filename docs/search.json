[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to the British Social Attitudes Survey",
    "section": "",
    "text": "Introduction\nThis repository contains the Quarto source files of the practical exercises for the Introduction to Social Attitudes Surveys UKDS Data Skills Module.\nA key aim of the BSA is to track the views and opinions of the public on national issues over time. The BSA questionnaire has core questions that are repeated in most years. These cover different topics such as politics, welfare, poverty, health, education, equalities, and employment. In addition, the interview questionnaire consists of various background and demographic questions. The rest of the questionnaire includes a series of non-core questions (modules) on a number of social, political, economic and moral topics, which are included in the survey less frequently.\nDirect links to the HTML pages of the exercises on GitHub Pages:\n\nR version:\n\nBasic population estimates with BSA data using R \nSurvey design informed inference with BSA data using R\n\nSPSS version:\n\nBasic population estimates with BSA data using SPSS \n Survey design informed inference with BSA data using SPSS",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingR.html",
    "href": "infer_w_survey_design_usingR.html",
    "title": "Using R with weights and survey design variables",
    "section": "",
    "text": "Getting started\nThis exercise is part of the ‘Introduction to the British Social Attitudes Survey (BSA)’ online module. In this exercise, we will practice statistical inference with data from the British Social Attitudes Survey (BSA) 2017 using weights and survey design variables.\nPlease note that at the time of writing this document only some of the BSA editions include survey design variables. For more information about inference from social surveys, including cases where weights and/or survey design variables are not available, please consult our guidelines.\nAnswers to the questions asked throughout the exercise can be found at the end of the page.\nData can be downloaded from the UK Data Service website following registration. Download the compressed folder, unzip and save it somewhere accessible on your computer.\nThe examples below assume that the dataset has been saved in a new folder named UKDS on your Desktop (Windows computers). The path would typically be C:\\Users\\YOUR_USER_NAME\\Desktop\\UKDS. Feel free to change it to the location that best suits your needs\nThe code below will need to be adjusted in order to match the location of the data on your computer.\nWe begin by loading the R packages needed for the exercise and set the working directory.\nlibrary(dplyr) ### Data manipulation functions\nlibrary(haven) ### Functions for importing data from commercial packages\nlibrary(Hmisc) ### Extra statistical functions\nlibrary(survey) ### Survey design functions\n\n### Setting up the working directory\n### Change the setwd() command  to match the location of the data on your computer \n### if required \n\nsetwd(\"C:\\Users\\Your_Username_here\\\")\n\ngetwd()\n\n# Opening the BSA dataset in SPSS format\nbsa17&lt;-read_spss(\"data/UKDA-8450-spss/spss/spss25/bsa2017_for_ukda.sav\")\n[1] C:\\Users\\Your_Username_here\\",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingR.html#identifying-the-survey-design-and-variables",
    "href": "infer_w_survey_design_usingR.html#identifying-the-survey-design-and-variables",
    "title": "Using R with weights and survey design variables",
    "section": "1. Identifying the survey design and variables",
    "text": "1. Identifying the survey design and variables\nWe first need to find out about the survey design that was used in the BSA 2017, and the design variables available in the dataset. Such information can usually be found in the documentation that comes together with the data under the mrdoc/pdf folder or in the data catalogue pages for the data on the UK Data Service website.\nQuestion 1\nWhat is the design that was used in this survey (i.e. how many sampling stages were there, and what were the units sampled). What were the primary sampling units; the strata (if relevant)?\nNow that we are a bit more familiar with the way the survey was designed, we need to try and identify the design variables we can include when producing estimates. The information can usually be found in the data documentation or the data dictionary available in the BSA documentation. \nQuestion 2\nWhat survey design variables are available? Are there any that are missing – if so which ones? What is the name of the weights variables?",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingR.html#specifying-the-survey-design",
    "href": "infer_w_survey_design_usingR.html#specifying-the-survey-design",
    "title": "Using R with weights and survey design variables",
    "section": "2. Specifying the survey design",
    "text": "2. Specifying the survey design\nWe need to tell R about the survey design. In practice this often means specifying the units selected at the initial sampling stage ie the Primary Sampling Units, as well as the strata. This is achieved with the svydesign() command. In effect this command creates a copy of the dataset with the survey design information attached, that can then subsequently be used for further estimation.\n\nbsa17.s&lt;-svydesign(ids=~Spoint,       ### Primary Sampling Units\n                   strata=~StratID,   ### Strata if stratified design\n                   weights=~WtFactor, ### Weights\n                   data=bsa17)        ### The dataset\nclass(bsa17.s)\n\n[1] \"survey.design2\" \"survey.design\" \n\nsummary(bsa17.s) ### Warning: very long output\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (372) clusters.\nsvydesign(ids = ~Spoint, strata = ~StratID, weights = ~WtFactor, \n    data = bsa17)\nProbabilities:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.2645  0.8288  1.0983  1.2386  1.6236  3.3318 \nStratum Sizes: \n           101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117\nobs         18  22  30  18  16  21  22  37  10  22  19  35  23  19  19  21  25\ndesign.PSU   2   2   3   2   2   2   2   3   2   3   2   3   2   2   2   2   2\nactual.PSU   2   2   3   2   2   2   2   3   2   3   2   3   2   2   2   2   2\n           118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134\nobs         12  12  32  40  25  21  23  26  23  18  34  23  20  29  39  19  30\ndesign.PSU   2   2   3   3   3   2   2   2   3   2   2   2   2   3   3   2   3\nactual.PSU   2   2   3   3   3   2   2   2   3   2   2   2   2   3   3   2   3\n           135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151\nobs         20  10  21  12  26  16  20  17  21  24  30  30  18  29  24  19  28\ndesign.PSU   2   2   2   2   3   2   2   2   2   3   2   3   2   3   2   3   2\nactual.PSU   2   2   2   2   3   2   2   2   2   3   2   3   2   3   2   3   2\n           152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168\nobs         18   8  23  33  14  23  17  39  13  22  16  19  21  18  26  13  14\ndesign.PSU   2   2   2   3   2   2   2   3   2   2   2   2   2   2   3   2   2\nactual.PSU   2   2   2   3   2   2   2   3   2   2   2   2   2   2   3   2   2\n           169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185\nobs         22  20   8  22  31  22  24  19  38  20  29  24  29  21  23  32  36\ndesign.PSU   2   2   2   2   2   2   2   2   3   2   2   2   3   2   2   3   3\nactual.PSU   2   2   2   2   2   2   2   2   3   2   2   2   3   2   2   3   3\n           186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202\nobs         24  22  43  38  38  47  34  15  22  35  17  20  20  21  21  43  35\ndesign.PSU   3   2   3   3   3   3   3   2   2   3   2   2   2   2   3   3   3\nactual.PSU   3   2   3   3   3   3   3   2   2   3   2   2   2   2   3   3   3\n           203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\nobs         28  25  19  18  28  15  21  30  24  33  24  22  30  24  44  18  26\ndesign.PSU   3   3   2   2   2   2   2   2   2   3   2   2   3   2   3   2   2\nactual.PSU   3   3   2   2   2   2   2   2   2   3   2   2   3   2   3   2   2\n           220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\nobs         22  28  20  27  34  33  41  24  23  26  17  23  36  20  45  32  27\ndesign.PSU   2   2   2   3   2   3   3   2   2   2   2   2   3   2   3   3   3\nactual.PSU   2   2   2   3   2   3   3   2   2   2   2   2   3   2   3   3   3\n           237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\nobs         33  25  39  31  29  33  20  43  22  24  26  29  37  22  27  25  43\ndesign.PSU   3   3   3   3   2   2   2   3   2   2   2   2   3   2   2   2   3\nactual.PSU   3   3   3   3   2   2   2   3   2   2   2   2   3   2   2   2   3\n           254 255 256 257 258 259\nobs          7  32  26  25  28  35\ndesign.PSU   2   3   2   2   2   3\nactual.PSU   2   3   2   2   2   3\nData variables:\n  [1] \"Sserial\"              \"Spoint\"               \"StratID\"             \n  [4] \"WtFactor\"             \"OldWt\"                \"GOR_ID\"              \n  [7] \"ABCVer\"               \"Country\"              \"househlde\"           \n [10] \"hhtypee\"              \"Rsex\"                 \"RAgeE\"               \n [13] \"RAgeCat\"              \"RAgeCat2\"             \"RAgecat3\"            \n [16] \"RAgecat4\"             \"RAgecat5\"             \"RSexAge\"             \n [19] \"RSexAge2\"             \"MarStat\"              \"Married\"             \n [22] \"legmarste\"            \"ChildHh\"              \"nch415e\"             \n [25] \"nch318e\"              \"hhch04e\"              \"hhch511e\"            \n [28] \"hhch1215e\"            \"hhch1617e\"            \"rch04e\"              \n [31] \"rch511e\"              \"rch1215e\"             \"rch1617e\"            \n [34] \"ownche\"               \"reconacte\"            \"RLastJob\"            \n [37] \"seconacte\"            \"Readpap\"              \"WhPaper\"             \n [40] \"paptype\"              \"TVNews\"               \"WebNews\"             \n [43] \"WNwSite1\"             \"WNwSite2\"             \"SMNews\"              \n [46] \"Internet\"             \"IntPers\"              \"MedResI\"             \n [49] \"SupParty\"             \"ClosePty\"             \"PartyIDN\"            \n [52] \"Partyid1\"             \"PartyId2\"             \"PartyID3\"            \n [55] \"PtyAlleg\"             \"Idstrng\"              \"Politics\"            \n [58] \"Coalitin\"             \"ConLabDf\"             \"VoteSyst\"            \n [61] \"ScotPar2\"             \"ECPolicy2\"            \"GovTrust\"            \n [64] \"Monarchy\"             \"MiEcono\"              \"MiCultur\"            \n [67] \"Spend1\"               \"Spend2\"               \"SocSpnd1\"            \n [70] \"SocSpnd2\"             \"SocSpnd3\"             \"SocSpnd4\"            \n [73] \"SocSpnd5\"             \"SocSpnd6\"             \"Dole\"                \n [76] \"TaxSpend\"             \"IncomGap\"             \"SRInc\"               \n [79] \"CMArran\"              \"RBGaran2\"             \"SepInvol\"            \n [82] \"SepServ\"              \"WkMent\"               \"WkPhys\"              \n [85] \"HProbRsp\"             \"PhsRetn\"              \"PhsRecov\"            \n [88] \"MntRetn\"              \"MntRecov\"             \"HCWork21\"            \n [91] \"HCWork22\"             \"HCWork23\"             \"HCWork24\"            \n [94] \"HCWork25\"             \"HCWork26\"             \"HCWork27\"            \n [97] \"HCWork28\"             \"HCWork29\"             \"NatFrEst\"            \n[100] \"FalseBn2\"             \"RepFrau3\"             \"RepWho1\"             \n[103] \"RepWho2\"              \"RepWho3\"              \"RepWho4\"             \n[106] \"RepWho5\"              \"RepWho6\"              \"RepWho7\"             \n[109] \"RepWho8\"              \"RepWho9\"              \"RepWho10\"            \n[112] \"WhyNRep1\"             \"WhyNRep2\"             \"WhyNRep3\"            \n[115] \"WhyNRep4\"             \"WhyNRep5\"             \"WhyNRep6\"            \n[118] \"WhyNRep7\"             \"WhyNRep8\"             \"WhyNRep9\"            \n[121] \"BFPnsh1\"              \"BFPnsh2\"              \"BFPnsh3\"             \n[124] \"BFPnsh4\"              \"BFPnsh5\"              \"BFPnsh6\"             \n[127] \"BFPnsh7\"              \"BFPnsh8\"              \"BFPnsh9\"             \n[130] \"BFPnsh10\"             \"BFPnsh11\"             \"AwrPB\"               \n[133] \"AdminPn2\"             \"LosofBen\"             \"AwrCRec\"             \n[136] \"GovDoBF\"              \"ImpHDoc\"              \"ImpHPar\"             \n[139] \"ImpHBeha\"             \"ImpHFam\"              \"ImpHEd\"              \n[142] \"ImpHJob\"              \"ImpHNeig\"             \"ImpHArea\"            \n[145] \"ImpHSafe\"             \"RespoHl2\"             \"HomsBult\"            \n[148] \"YSBEmpl\"              \"YSBTrans\"             \"YSBGreen\"            \n[151] \"YSBSch\"               \"YSBAfRnt\"             \"YSBAfOwn\"            \n[154] \"YSBDesig\"             \"YSBShops\"             \"YSBMedic\"            \n[157] \"YSBLibry\"             \"YSBLeis\"              \"YSBFinan\"            \n[160] \"YSBOther\"             \"YSBDeps\"              \"YSBNone\"             \n[163] \"HousGSD\"              \"Buldres\"              \"EdSpnd1c\"            \n[166] \"EdSpnd2c\"             \"VocVAcad\"             \"ATTD151\"             \n[169] \"ATTD152\"              \"ATTD153\"              \"ATTD154\"             \n[172] \"ATTD155\"              \"ATTD156\"              \"ATTD157\"             \n[175] \"ATTD158\"              \"ATTD81\"               \"ATTD82\"              \n[178] \"ATTD83\"               \"ATTD84\"               \"ATTD85\"              \n[181] \"ATTD86\"               \"ATTD87\"               \"ATTD88\"              \n[184] \"GCSEFur\"              \"GCSEWrk\"              \"ALevFur\"             \n[187] \"ALevWrk\"              \"HEdOpp\"               \"ChLikUn2\"            \n[190] \"HEFee\"                \"FeesUni\"              \"FeesSub\"             \n[193] \"Himp\"                 \"PREVFR\"               \"TRFPB6U\"             \n[196] \"TRFPB9U\"              \"TrfPb10u\"             \"TrfConc1\"            \n[199] \"DRIVE\"                \"carnume\"              \"CycDang\"             \n[202] \"Bikeown2\"             \"BikeRid\"              \"TRAVEL1\"             \n[205] \"TRAVEL2\"              \"TRAVEL3\"              \"TRAVEL4a\"            \n[208] \"TRAVEL6\"              \"airtrvle\"             \"CCTrans1\"            \n[211] \"CCTrans2\"             \"CCTrans3\"             \"CCTrans4\"            \n[214] \"CCTrans5\"             \"CCTrans6\"             \"CCTrans7\"            \n[217] \"CCTrans8\"             \"CCTrans9\"             \"CCALowE\"             \n[220] \"CCACar\"               \"CCAPLANE\"             \"CCBELIEV\"            \n[223] \"EUBrld\"               \"EUExInf2\"             \"EUExUne2\"            \n[226] \"EUExIm2\"              \"EUExEco2\"             \"EUImpSov\"            \n[229] \"LeavEUI\"              \"EUconte\"              \"EUcontu\"             \n[232] \"EUconth\"              \"EULtop1\"              \"EULtop2\"             \n[235] \"EULtop3\"              \"NHSSat\"               \"WhySat1\"             \n[238] \"WhySat2\"              \"WhySat3\"              \"WhySat4\"             \n[241] \"WhySat5\"              \"WhySat6\"              \"WhySat7\"             \n[244] \"WhySat8\"              \"WhySat9\"              \"WhySat10\"            \n[247] \"WhyDis1\"              \"WhyDis2\"              \"WhyDis3\"             \n[250] \"WhyDis4\"              \"WhyDis5\"              \"WhyDis6\"             \n[253] \"WhyDis7\"              \"WhyDis8\"              \"WhyDis9\"             \n[256] \"WhyDis10\"             \"GPSat\"                \"DentSat\"             \n[259] \"InpatSat\"             \"OutpaSat\"             \"AESat\"               \n[262] \"CareSat3\"             \"NHSFProb\"             \"NHS5yrs\"             \n[265] \"NHSNx5Yr\"             \"NHSAcc\"               \"NHSImp\"              \n[268] \"AEtravel\"             \"CareNee2\"             \"PaySocia\"            \n[271] \"CarePa2\"              \"SocFutur\"             \"Tranneed\"            \n[274] \"Prejtran\"             \"PMS\"                  \"HomoSex\"             \n[277] \"SSRel\"                \"RSuperv\"              \"rocsect2e\"           \n[280] \"REmpWork\"             \"REmpWrk2\"             \"SNumEmp\"             \n[283] \"WkJbTim\"              \"ESrJbTim\"             \"SSrJbTim\"            \n[286] \"WkJbHrsI\"             \"ExPrtFul\"             \"EJbHrCaI\"            \n[289] \"SJbHrCaI\"             \"RPartFul\"             \"S2PartFl\"            \n[292] \"Remplyee\"             \"UnionSA\"              \"TUSAEver\"            \n[295] \"NPWork10\"             \"RES2010\"              \"RES2000\"             \n[298] \"SLastJb2\"             \"S2Employ\"             \"S2Superv\"            \n[301] \"S2ES2010\"             \"S2ES2000\"             \"rjbtype\"             \n[304] \"REconSum\"             \"REconPos\"             \"RNSEGGrp\"            \n[307] \"RNSocCl\"              \"RNSSECG\"              \"RClass\"              \n[310] \"RClassGp\"             \"RSIC07GpE\"            \"seconsum\"            \n[313] \"S2NSEGGp\"             \"S2NSSECG\"             \"S2NSocCl\"            \n[316] \"S2Class\"              \"S2ClassG\"             \"WAGMIN\"              \n[319] \"RESPPAY\"              \"TRCURJM\"              \"TRCURJN\"             \n[322] \"TRMRSJM\"              \"TRMRSJN\"              \"TRDIFJM\"             \n[325] \"TRDIFJN\"              \"PHOURS\"               \"REGHOUR\"             \n[328] \"WRKCON\"               \"JBMRESP\"              \"JBMWH1\"              \n[331] \"JBMWH2\"               \"JBMWH3\"               \"JBMWH4\"              \n[334] \"JBMWH5\"               \"JBMWH6\"               \"JBMWH7\"              \n[337] \"JBMWH8\"               \"FLEXHRS\"              \"MgCWld\"              \n[340] \"MgMWld\"               \"ChgAsJb1\"             \"ChgAsJb2\"            \n[343] \"ChgAsJb3\"             \"ChgJbTim\"             \"RetExp\"              \n[346] \"RetExpb\"              \"DVRetAge\"             \"PenKnow2\"            \n[349] \"RPenSrc1\"             \"RPenSrc2\"             \"RPenSrc3\"            \n[352] \"whrbrne\"              \"NatIdGB\"              \"NatId\"               \n[355] \"tenure2e\"             \"RentPrf1\"             \"HAWhat\"              \n[358] \"HAgdbd\"               \"HANotFM\"              \"LikeHA\"              \n[361] \"HAYwhy\"               \"HANwhy\"               \"HsDepnd\"             \n[364] \"ResPres\"              \"ReligSum\"             \"RlFamSum\"            \n[367] \"ChAttend\"             \"bestnatu2\"            \"raceori4\"            \n[370] \"DisNew2\"              \"DisAct\"               \"DisActDV\"            \n[373] \"Knowdis1\"             \"Knowdis2\"             \"Knowdis3\"            \n[376] \"Knowdis4\"             \"Knowdis5\"             \"Knowdis6\"            \n[379] \"Knowdis7\"             \"DisPrj\"               \"Dis100\"              \n[382] \"tea3\"                 \"HEdQual\"              \"HEdQual2\"            \n[385] \"HEdQual3\"             \"EUIdent\"              \"BritID2\"             \n[388] \"Voted\"                \"Vote\"                 \"EURefV2\"             \n[391] \"EUVOTWHO\"             \"EURefb\"               \"AnyBN3\"              \n[394] \"MainInc5\"             \"HHIncD\"               \"HHIncQ\"              \n[397] \"REarnD\"               \"REarnQ\"               \"SelfComp\"            \n[400] \"knwbdri\"              \"knwexec\"              \"knwclea\"             \n[403] \"knwhair\"              \"knwhr\"                \"knwlaw\"              \n[406] \"knwmech\"              \"knwnurs\"              \"knwpol\"              \n[409] \"knwtchr\"              \"incdiffs\"             \"incdsml\"             \n[412] \"govldif\"              \"socblaz\"              \"whoprvhc\"            \n[415] \"whoprvca\"             \"actgrp\"               \"actpol\"              \n[418] \"actchar\"              \"govnosa2\"             \"hhldjob\"             \n[421] \"hhmsick\"              \"hdown\"                \"hadvice\"             \n[424] \"hsococc\"              \"hlpmny\"               \"hlpjob\"              \n[427] \"hlpadmin\"             \"hlplive\"              \"hlpill\"              \n[430] \"lckcomp\"              \"isolate\"              \"leftout\"             \n[433] \"peopadvt\"             \"peoptrst\"             \"trstcrts\"            \n[436] \"trstprc\"              \"helpeldy\"             \"helpslf1\"            \n[439] \"helpfrnd\"             \"fampress\"             \"reltdemd\"            \n[442] \"ffrangr\"              \"eatout\"               \"newfrnd\"             \n[445] \"pplcont\"              \"pplftf\"               \"parcont\"             \n[448] \"sibcon2\"              \"chdcon2\"              \"othcont\"             \n[451] \"frndcont\"             \"contint\"              \"ltsgnhth\"            \n[454] \"depres\"               \"diffpile\"             \"acgoals\"             \n[457] \"lifesat2\"             \"makeem\"               \"langgs\"              \n[460] \"helpslf2\"             \"payback\"              \"domconv\"             \n[463] \"sitwhr\"               \"hmecont\"              \"religcon\"            \n[466] \"spseedu\"              \"ben3000\"              \"ben3000d\"            \n[469] \"falcatch\"             \"uniaff\"               \"unicar\"              \n[472] \"bothearn\"             \"sexrole\"              \"womworka\"            \n[475] \"womworkb\"             \"parlvmf2\"             \"gendwrk\"             \n[478] \"gendmath\"             \"gendcomp\"             \"sxbstrm\"             \n[481] \"sxbintm\"              \"sxbstrw\"              \"sxbintw\"             \n[484] \"sxblaw\"               \"sxbprov\"              \"sxboffb\"             \n[487] \"sxbnoone\"             \"sxboth\"               \"sxbcc\"               \n[490] \"carwalk2\"             \"carbus2\"              \"carbike2\"            \n[493] \"shrtjrn\"              \"plnallow\"             \"plnterm\"             \n[496] \"plnenvt\"              \"plnuppri\"             \"cartaxhi\"            \n[499] \"carallow\"             \"carreduc\"             \"carnod2\"             \n[502] \"carenvdc\"             \"resclose\"             \"res20mph\"            \n[505] \"resbumps\"             \"ddnodrv\"              \"ddnklmt\"             \n[508] \"specamsl\"             \"specammo\"             \"specamtm\"            \n[511] \"speedlim\"             \"speavesc\"             \"mobdsafe\"            \n[514] \"mobddang\"             \"mobdban\"              \"mobdlaw\"             \n[517] \"eutrdmv\"              \"consvfa\"              \"labrfa\"              \n[520] \"libdmfa\"              \"ukipfa\"               \"rthdswa2\"            \n[523] \"rthdsaw2\"             \"rthdsca2\"             \"rthdssa2\"            \n[526] \"rthdsprd\"             \"eqrdisab\"             \"nhsoutp2\"            \n[529] \"nhsinp2\"              \"bodimr\"               \"bodimop\"             \n[532] \"girlwapp\"             \"tprwrong2\"            \"eulunem\"             \n[535] \"eulimm\"               \"eulecon\"              \"eulwork\"             \n[538] \"eullowi\"              \"eulmlow\"              \"eulnhs\"              \n[541] \"jbernmny\"             \"jbenjoy\"              \"topupchn\"            \n[544] \"topupnch\"             \"topuplpa\"             \"worknow\"             \n[547] \"losejob\"              \"jbgdcurr\"             \"robots\"              \n[550] \"robown\"               \"voteduty\"             \"welfhelp\"            \n[553] \"morewelf\"             \"unempjob\"             \"sochelp\"             \n[556] \"dolefidl\"             \"welffeet\"             \"damlives\"            \n[559] \"proudwlf\"             \"redistrb\"             \"BigBusnn\"            \n[562] \"wealth\"               \"richlaw\"              \"indust4\"             \n[565] \"tradvals\"             \"stifsent\"             \"deathapp\"            \n[568] \"obey\"                 \"wronglaw\"             \"censor\"              \n[571] \"leftrigh\"             \"libauth\"              \"welfare2\"            \n[574] \"libauth2\"             \"leftrig2\"             \"welfgrp\"             \n[577] \"eq_inc_deciles\"       \"eq_inc_quintiles\"     \"eq_bhcinc2_deciles\"  \n[580] \"eq_bhcinc2_quintiles\"",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingR.html#mean-age-and-its-95-confidence-interval",
    "href": "infer_w_survey_design_usingR.html#mean-age-and-its-95-confidence-interval",
    "title": "Using R with weights and survey design variables",
    "section": "3. Mean age and its 95% confidence interval",
    "text": "3. Mean age and its 95% confidence interval\nWe can now produce a first set of estimates using this information and compare them with those we would have got without accounting for the survey design. We will compute the average (ie mean) age of respondents in the sample. We will need to use svymean()\n\nsvymean(~RAgeE,bsa17.s)\n\n        mean     SE\nRAgeE 48.313 0.4236\n\n\nBy default svymean() computes the standard error of the mean. We need to\nembed it within confint() in order to get a confidence interval.\n\nconfint(svymean(~RAgeE,bsa17.s)) ### Just the confidence interval...\n\n         2.5 %  97.5 %\nRAgeE 47.48289 49.1433\n\nround(\n  c(\n    svymean(~RAgeE,bsa17.s),\n    confint(svymean(~RAgeE,bsa17.s))\n    ),\n  1) ### ... Or both, rounded\n\nRAgeE             \n 48.3  47.5  49.1 \n\n\nWhat difference would it make to the estimates and 95% CI to compute respectively, an unweighted mean, as well as a weighted mean without accounting for the survey design?\nThere are different ways of computing ‘naive estimates’ in R. Below we demonstrate how to do it ´by hand’ for greater transparency.\nBase R provides a function for computing the variance of a variable: var(). Since we know that:\n\nThe standard deviation of the mean is the square root of its variance\nThe standard error of a sample mean is its standard deviation divided by the square root of the sample size\nA 95% confidence interval is the sample mean respectively minus and plus 1.96 times its standard error. It is then relatively straightforward to compute unweighted and ‘casually weighted’ confidences intervals for the mean.\n\n\n### Unweighted means and CI\nu.m&lt;- mean(bsa17$RAgeE)\nu.se&lt;-sqrt(var(bsa17$RAgeE))/sqrt(length(bsa17$RAgeE))\nu.ci&lt;-c(u.m - 1.96*u.se,u.m + 1.96*u.se)\nround(c(u.m,u.ci),1)\n\n[1] 52.2 51.6 52.8\n\n### Weighted means and CI without survey design\nw.m&lt;- wtd.mean(bsa17$RAgeE,bsa17$WtFactor)\nw.se&lt;-sqrt(wtd.var(bsa17$RAgeE,bsa17$WtFactor))/sqrt(length(bsa17$RAgeE))\nw.ci&lt;-c(w.m - 1.96*w.se,w.m + 1.96*w.se)\nround(c(w.m,w.ci),1)\n\n[1] 48.3 47.7 48.9\n\n\nQuestion 3\nWhat are the consequences of not accounting for the sample design; not using weights and accounting for the sample design when:\n- inferring the mean value of the population age?\n- inferring the uncertainty of our estimate of the population age?",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingR.html#computing-a-proportion-and-its-95-confidence-interval",
    "href": "infer_w_survey_design_usingR.html#computing-a-proportion-and-its-95-confidence-interval",
    "title": "Using R with weights and survey design variables",
    "section": "4. Computing a proportion and its 95% confidence interval",
    "text": "4. Computing a proportion and its 95% confidence interval\nWe can now similarly estimate the distribution of a categorical variable in the population by computing proportions (or percentages), for instance, the proportion of people who declare themselves interested in politics. This is the Politics variable. It has five categories that we are going to recode into ‘Significantly’ (interested) and ‘Not’ (significantly), for simplicity.\nThe BSA regards ‘don’t know’ and ‘refusal’ responses as valid but since in this case there is only one ‘don’t know’ and no ‘refusal’, we can safely ignore these categories and recode them as system missing. As before, we prefer using xtabs() over table() as it allows us to ignore unused factor levels.\n\nattr(bsa17$Politics,\"label\")     ### Phrasing of the question\n\n[1] \"How much interest do you have in politics?\"\n\nxtabs(~as_factor(Politics),\n      data=bsa17,\n      drop.unused.levels = T) ### Sample distribution\n\nas_factor(Politics)\n... a great deal,      quite a lot,             some,    not very much, \n              739               982              1179               708 \n or, none at all?        Don`t know \n              379                 1 \n\nbsa17$Politics.s&lt;-ifelse(bsa17$Politics==1 | bsa17$Politics==2,\n                         \"Significantly\",NA)\nbsa17$Politics.s&lt;-ifelse(bsa17$Politics&gt;=3 & bsa17$Politics&lt;=5,\n                         \"Not Interested\",bsa17$Politics.s)\nbsa17$Politics.s&lt;-as.factor(bsa17$Politics.s)\n\nrbind(xtabs(~as_factor(Politics.s),\n      data=bsa17,\n      drop.unused.levels = T) ,\n      round(\n        100*prop.table(\n          xtabs(~as_factor(Politics),\n          data=bsa17,\n          drop.unused.levels = T) \n          ),\n        1)\n)\n\n     ... a great deal, quite a lot,  some, not very much, or, none at all?\n[1,]            2266.0       1721.0 2266.0         1721.0           2266.0\n[2,]              18.5         24.6   29.6           17.8              9.5\n     Don`t know\n[1,]       1721\n[2,]          0\n\n\nChanges in a data frame are not automatically transferred into svydesign objects used for inferences. We therefore need to recreate it each time we create or recode a variable.\n\nrbind(round(xtabs(WtFactor~Politics.s,bsa17),\n            1),\n      round(100*\n              prop.table(\n                xtabs(WtFactor~Politics.s,bsa17))\n            ,1)\n)\n\n     Not Interested Significantly\n[1,]         2270.6        1715.2\n[2,]           57.0          43.0\n\nbsa17.s&lt;-svydesign(ids=~Spoint,      \n                   strata=~StratID, \n                   weights=~WtFactor,\n                   data=bsa17)\n\nrbind(round(svytable(~Politics.s,\n                     bsa17.s),1),\n      round(100*prop.table(\n        svytable(~Politics.s,\n                 bsa17.s)),1)\n)\n\n     Not Interested Significantly\n[1,]         2270.6        1715.2\n[2,]           57.0          43.0\n\n\nAs with the mean of age earlier, we can see that the weighted and unweighted point estimates of the proportion of respondents significantly interested in politics differ, even if slightly, and that weighted point estimates do not differ irrespective of the survey design being accounted for.\nLet us now examine the confidence intervals of these proportions. Traditional statistical software usually compute these without telling us about the underlying computations going on. By contrast, doing this in R requires more coding, but in the process we gain a better understanding of what is actually estimated.\nConfidence intervals for proportion of categorical variables are usually computed as a sequence of binomial/dichotomic estimations – ie one for each category. In R this needs to be specified explicitly via the svyciprop() and I() functions. The former actually computes the proportion and its confidence interval (by default 95%), whereas the latter allows us to define the category we are focusing on (in case of non dichotomic variable).\n\nsvyciprop(~I(Politics.s==\"Significantly\"),\n          bsa17.s)\n\n                                        2.5% 97.5%\nI(Politics.s == \"Significantly\") 0.430 0.411 0.450\n\nround(100*\n        c(prop.table(\n          svytable(~Politics.s,bsa17.s))[2],\nattr(svyciprop(~I(Politics.s==\"Significantly\"),\n               bsa17.s),\"ci\")),1\n)\n\nSignificantly          2.5%         97.5% \n         43.0          41.1          45.0 \n\n\nQuestion 4\nWhat is the proportion of respondents aged 17-34 in the sample, as well as its 95% confidence interval? You can use RAgecat5",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingR.html#domain-ie-subpopulation-estimates",
    "href": "infer_w_survey_design_usingR.html#domain-ie-subpopulation-estimates",
    "title": "Using R with weights and survey design variables",
    "section": "5. Domain (ie subpopulation) estimates",
    "text": "5. Domain (ie subpopulation) estimates\nComputing estimates for specific groups of a sample (for example the average age of people who reported being interested in politics) is not much more difficult than doing it for the sample as a whole. However doing it as part of an inferential analysis requires some caution. Calculating weighted estimates for a subpopulation, amounts to computing second order estimates ie an estimate for a group whose size needs to be estimated first. Therefore, attempting this while leaving out of the rest of the sample might yield incorrect results. This is why using survey design informed functions is particularly recommended in such cases.\nThe survey package functionsvyby() makes such domain estimation relatively straightforward. For instance, if we would like to compute the mean age of BSA respondents by Government Office Regions, we need to specify:\n\nThe outcome variable whose estimate we want to compute: ie RAgeE\nThe grouping variable(s) GOR_ID\nThe estimate function we are going to use here: svymean, the same as we used before\nAnd the type of type of variance estimation we would like to see displayed ie standard errors or confidence interval\n\n\nbsa17$gor.f&lt;-as_factor(bsa17$GOR_ID)\nbsa17.s&lt;-svydesign(ids=~Spoint, \n                   strata=~StratID, \n                   weights=~WtFactor,\n                   data=bsa17)\n\nround(svyby(~RAgeE,\n            by=~gor.f,\n            svymean,\n            design=bsa17.s,\n            vartype = \"ci\")[-1],1)\n\n                           RAgeE ci_l ci_u\nA North East                46.1 43.6 48.6\nB North West                49.6 47.3 52.0\nD Yorkshire and The Humber  48.0 45.2 50.8\nE East Midlands             48.6 45.9 51.3\nF West Midlands             48.1 45.0 51.2\nG East of England           49.0 46.0 52.0\nH London                    45.0 43.0 46.9\nJ South East                48.0 45.1 50.8\nK South West                53.4 51.5 55.2\nL Wales                     49.1 45.1 53.1\nM Scotland                  47.3 44.7 50.0\n\n\nNote: we used [-1] from the object created by svyby() in order to remove a column with alphanumeric values (the region names), so that we could round the results without getting an error.\nOur inference seem to suggest that the population in London is among the youngest in the country, and that those in the South West are among the oldest – their respective 95% confidence intervals do not overlap. We should not feel so confident about differences between London and the South East for example, as the CIs partially overlap.\nWe can follow a similar approach with proportions: we just need to specify the category of the variable we are interested in as an outcome, for instance respondents who are significantly interested in politics, and replace svymean by svyciprop.\n\nround(\n      100*\n      svyby(~I(Politics.s==\"Significantly\"),\n            by=~gor.f,\n            svyciprop,\n            design=bsa17.s,\n            vartype = \"ci\")[-1],\n            1)\n\n                           I(Politics.s == \"Significantly\") ci_l ci_u\nA North East                                           33.4 26.6 40.9\nB North West                                           42.1 36.3 48.2\nD Yorkshire and The Humber                             35.6 29.1 42.6\nE East Midlands                                        36.9 32.9 41.1\nF West Midlands                                        36.3 31.5 41.5\nG East of England                                      47.2 41.4 53.1\nH London                                               54.2 47.2 61.1\nJ South East                                           44.6 38.7 50.8\nK South West                                           46.5 39.4 53.8\nL Wales                                                38.6 27.7 50.7\nM Scotland                                             42.7 36.0 49.8\n\n\nQuestion 5\nWhat is the 95% confidence interval for the proportion of people interested in politics in the South West? Is the proportion likely to be different in London? In what way? What is the region of the UK for which the precision of the estimates is likely to be the smallest?\nWhen using svyby(), we can define domains or subpopulations with several variables, not just one. For example, we could have looked at gender differences in political affiliations by regions. However, as the size of subgroups decrease, so does the precision of the estimates as their confidence interval widens, to a point where their substantive interest is not meaningful anymore.\nQuestion 6\nUsing interest in politics as before, and three category age RAgecat5 (which you may want to recode as a factor in order to improve display clarity): \n- Produce a table of results showing the proportion of respondents significantly interested in Politics by age group\n- Assess whether the age difference in interest for politics is similar for each gender?\n- Based on the data, is it fair to say that men aged under 35 tend to be more likely to declare themselves interested in politics than women aged 55 and above?",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingR.html#answers",
    "href": "infer_w_survey_design_usingR.html#answers",
    "title": "Using R with weights and survey design variables",
    "section": "Answers",
    "text": "Answers\nQuestion 1 The 2017 BSA is a three stage stratified random survey, with postcode sectors, adresses and individuals as the units selected at each stage. Primary sampling units were furthermore stratified according to geographies (sub regions), population density, and proportion of owner-occupiers. Sampling rate was proportional to the size of postcode sectors (ie number of addresses)\nQuestion 2 From the Data Dictionary it appears that the primary sampling units (sub regions) are identified bySpoint and the strata byStratID. The weights variable isWtFactor. Addresses are not provided but could be approximated with a household identifier.\nQuestion 3 Not using weights would make us overestimate the mean age in the population (of those aged 16+) by about 4 years. This is likely to be due to the fact that older respondents are more likely to take part to surveys. Using survey design variables does not alter the value of the estimated population mean. However, not accounting for them would lead us to overestimate the precision/underestimate the uncertainty of our estimate with a narrower confidence interval – by about plus and minus 2 months .\nQuestion 4 The proportion of 17-25 year old in the sample is 28.5 and its 95%confidence interval 26.5, 30.6\nQuestion 5 The 95% confidence interval for the proportion of people interested in politics in the South West is 39.4, 53.8. By contrast, it is likely to be 47.2, 61.1 in London. The region with the lowest precision of estimates (ie the widest confidence interval) is Wales, with a 23 percentage point difference between the upper and lower bounds of the confidence interval.\nQuestion 6\n\nbsa17$RAgecat5.f&lt;-as_factor(bsa17$RAgecat5)\nbsa17$Rsex.f&lt;-as_factor(bsa17$Rsex)\n\n\nbsa17.s&lt;-svydesign(ids=~Spoint, \n                   strata=~StratID, \n                   weights=~WtFactor,\n                   data=bsa17)\n\nround(\n      100*\n      svyby(~I(Politics.s==\"Significantly\"),\n            by=~RAgecat5.f+Rsex.f,\n            svyciprop,\n            design=bsa17.s,\n            vartype = \"ci\")[c(-8,-4),c(-2,-1)],\n      1)\n\n             I(Politics.s == \"Significantly\") ci_l ci_u\n17-34.Male                               42.9 37.7 48.2\n35-54.Male                               50.8 46.6 54.9\n55+.Male                                 57.8 53.9 61.6\n17-34.Female                             26.3 22.0 31.1\n35-54.Female                             34.1 30.6 37.8\n55+.Female                               43.0 39.6 46.5\n\n\nOlder respondents both male and female tend to be more involved in politics than younger ones.\nThe confidence intervals for the proportion of men under 35 and women above 55 interested in politics overlap; it is unlikely that they differ in the population.",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using R with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "Population estimates using the BSAS with R.html",
    "href": "Population estimates using the BSAS with R.html",
    "title": "Basic population estimates with BSA data using R",
    "section": "",
    "text": "Getting started\nThis exercise is part of the ‘Introduction to the British Social Attitudes Survey (BSA)’ online module. In the exercise, we examine data from the 2020 British Social Attitudes survey to find out:\nAnswers to the questions asked throughout the exercise can be found at the end of the page.\nData can be downloaded from the UK Data Service website following registration. Download the compressed folder, unzip and save it somewhere accessible on your computer.\nThe examples below assume that the dataset has been saved in a new folder named UKDS on your Desktop (Windows computers). The path would typically be C:\\Users\\YOUR_USER_NAME\\Desktop\\UKDS. Feel free to change it to the location that best suits your needs.\nWe begin by loading the R packages needed for the exercise and set the working directory.\nlibrary(dplyr) ### Data manipulation functions\nlibrary(haven) ### Functions for importing data from \n               ### commercial packages\nlibrary(Hmisc) ### Extra statistical functions\n\n### Setting up the working directory\n### Please adjust the setwd() command  below   \n### to match the location of the data on your computer \n\nsetwd(\"C:\\Users\\Your_Username_here\\\")\n\ngetwd()\n[1] C:\\Users\\Your_Username_here\\\nWe then open the BSA dataset in SPSS format. Stata or tab-delimited format can also be used.\nbsa20&lt;-read_spss(\n       'UKDA-9005-spss/spss/spss25/bsa2020_archive.sav'\n       )",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic population estimates with BSA data using R</span>"
    ]
  },
  {
    "objectID": "Population estimates using the BSAS with R.html#explore-the-dataset",
    "href": "Population estimates using the BSAS with R.html#explore-the-dataset",
    "title": "Basic population estimates with BSA data using R",
    "section": "1. Explore the dataset",
    "text": "1. Explore the dataset\nStart by getting an overall feel for the data. Use the code below to produce a summary of all the variables in the dataset.\n\n### Gives the number of rows (observations) \n### and columns (variables)\ndim(bsa20) \n\n[1] 3964  210\n\n ### List variable names in their actual \n ### order in the dataset\nnames(bsa20)\n\n  [1] \"serial\"       \"QnrVersion\"   \"RespSx2cat\"   \"RespAgeE\"     \"MarStat6\"    \n  [6] \"REconFW01\"    \"REconFW02\"    \"REconFW03\"    \"REconFW04\"    \"REconFW05\"   \n [11] \"REconFW06\"    \"REconFW07\"    \"REconFW08\"    \"REconFW09\"    \"REconFW10\"   \n [16] \"REconFW11\"    \"EMPSTAT\"      \"Employ\"       \"Superv\"       \"EmpOCC\"      \n [21] \"TenureE\"      \"SupParty\"     \"ClosePty\"     \"PARTYFW\"      \"Idstrng\"     \n [26] \"RemLea\"       \"RemLeaCl\"     \"RemLeaSt\"     \"Politics\"     \"ConLabDf\"    \n [31] \"VoteDuty\"     \"SocTrust\"     \"EngParl\"      \"ScotPar2\"     \"ECPolicy2\"   \n [36] \"Spend1\"       \"Spend2\"       \"SocBen1\"      \"SOCBEN2\"      \"DOLE\"        \n [41] \"TAXSPEND\"     \"WkMent\"       \"WkPhys\"       \"HProbRsp\"     \"PhsRetn\"     \n [46] \"PhsRecov\"     \"MntRetn\"      \"MntRecov\"     \"HCWork21\"     \"HCWork22\"    \n [51] \"HCWork23\"     \"HCWork24\"     \"HCWork25\"     \"HCWork26\"     \"HCWork28\"    \n [56] \"HCWork29\"     \"HCWork213\"    \"HCWork214\"    \"HCWork215\"    \"HCWork27\"    \n [61] \"CMtUnmar1\"    \"CMtUnmar2\"    \"CMtUnmar3\"    \"CMtUnmar4\"    \"CMtUnmar5\"   \n [66] \"CMtUnmar6\"    \"CMtUnmar7\"    \"CMtUnmar8\"    \"CMtUnmar9\"    \"CMtUnmar10\"  \n [71] \"CMtmar1\"      \"CMtmar2\"      \"CMtmar3\"      \"CMtmar4\"      \"CMtmar5\"     \n [76] \"CMtmar6\"      \"CMtmar7\"      \"CMtmar8\"      \"CMtmar9\"      \"CMtmar10\"    \n [81] \"ChCoSupp\"     \"ChMIncM\"      \"ChMIncF\"      \"ChMCont\"      \"RBGaran2\"    \n [86] \"RBGGov\"       \"DigPCUn\"      \"DigPCctl\"     \"DigPCcon\"     \"DigPCrsk\"    \n [91] \"DigGVun\"      \"DigGVctl\"     \"DigGVcon\"     \"DigGVrsk\"     \"DigPro\"      \n [96] \"NHSSat\"       \"WkHmNow\"      \"WkHmJan\"      \"CovWkc\"       \"CovNoWkc\"    \n[101] \"CovWkr1\"      \"CovWkr2\"      \"CovWkr3\"      \"CovWkr4\"      \"CovWkr5\"     \n[106] \"CovWkr6\"      \"CovWk1\"       \"CovWk2\"       \"CovWk3\"       \"GovtWork\"    \n[111] \"GovTrust\"     \"CLRTRUST\"     \"MPsTrust\"     \"LoseTch\"      \"VoteIntr\"    \n[116] \"PtyNMat2\"     \"PolPart01\"    \"PolPart02\"    \"PolPart03\"    \"PolPart04\"   \n[121] \"PolPart05\"    \"PolPart06\"    \"PolPart07\"    \"PolPart08\"    \"PolPart09\"   \n[126] \"PolPart10\"    \"PolPart11\"    \"REFHANG\"      \"RefSyst\"      \"UnempJob\"    \n[131] \"SocHelp\"      \"DoleFidl\"     \"WelfFeet\"     \"welfhelp\"     \"morewelf\"    \n[136] \"damlives\"     \"proudwlf\"     \"Redistrb\"     \"BigBusnN\"     \"Wealth\"      \n[141] \"RichLaw\"      \"Indust4\"      \"TradVals\"     \"StifSent\"     \"DeathApp\"    \n[146] \"Obey\"         \"WrongLaw\"     \"Censor\"       \"NatIdGB\"      \"ChAttend\"    \n[151] \"DisNew2\"      \"DisAct\"       \"HEdQual2\"     \"HhldEdu\"      \"EURefV2\"     \n[156] \"EUVOTWHO\"     \"EURefb\"       \"Voted\"        \"Vote\"         \"Anybn3\"      \n[161] \"HHincome\"     \"Maininc5\"     \"REarn\"        \"HIncDif4\"     \"RetExp\"      \n[166] \"RetExpb\"      \"FutrWrk\"      \"PenKnow2\"     \"PenExp2\"      \"PenComp\"     \n[171] \"PenIntr\"      \"INFORET3\"     \"WkPKnw\"       \"WKPSav\"       \"WkPSpn\"      \n[176] \"WPSvUs\"       \"WPSvWw\"       \"WPSvEas\"      \"PrPKnw\"       \"PrPSav\"      \n[181] \"PrPSpn\"       \"PrPSvUs\"      \"PrPSvWW\"      \"PrPSvEas\"     \"NCOutcome\"   \n[186] \"Ragecat\"      \"Ragecat20\"    \"DisActDV\"     \"leftrigh\"     \"libauth\"     \n[191] \"welfare2\"     \"libauth2\"     \"leftrig2\"     \"welfgrp\"      \"REconAct20\"  \n[196] \"REconSum20\"   \"RaceOri4\"     \"LegMarStE\"    \"HhlAdGpd\"     \"HhlChlGpd\"   \n[201] \"BestNatU2\"    \"RetirAg3\"     \"ReligSum20\"   \"RlFamSum20\"   \"EmplStatDV\"  \n[206] \"RClassGP\"     \"serialh\"      \"GOR\"          \"gor2\"         \"BSA20_wt_new\"\n\n### Displays the first five\n### lines of a data frame\n\nhead(bsa20)  \n\n# A tibble: 6 × 210\n  serial   QnrVersion RespSx2cat RespAgeE MarStat6 REconFW01 REconFW02 REconFW03\n  &lt;dbl+lb&gt; &lt;dbl+lbl&gt;  &lt;dbl+lbl&gt;  &lt;dbl+lb&gt; &lt;dbl+lb&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;\n1 3.21e9   1 [Versio… 2 [Male]   70       5 [Divo… 0 [No]    0 [No]    0 [No]   \n2 3.21e9   1 [Versio… 2 [Male]   66       1 [Marr… 0 [No]    0 [No]    0 [No]   \n3 3.21e9   1 [Versio… 1 [Female] 64       1 [Marr… 0 [No]    0 [No]    0 [No]   \n4 3.21e9   1 [Versio… 2 [Male]   43       1 [Marr… 0 [No]    0 [No]    1 [Yes]  \n5 3.21e9   1 [Versio… 1 [Female] 38       1 [Marr… 0 [No]    0 [No]    1 [Yes]  \n6 3.21e9   1 [Versio… 2 [Male]   77       1 [Marr… 0 [No]    0 [No]    0 [No]   \n# ℹ 202 more variables: REconFW04 &lt;dbl+lbl&gt;, REconFW05 &lt;dbl+lbl&gt;,\n#   REconFW06 &lt;dbl+lbl&gt;, REconFW07 &lt;dbl+lbl&gt;, REconFW08 &lt;dbl+lbl&gt;,\n#   REconFW09 &lt;dbl+lbl&gt;, REconFW10 &lt;dbl+lbl&gt;, REconFW11 &lt;dbl+lbl&gt;,\n#   EMPSTAT &lt;dbl+lbl&gt;, Employ &lt;dbl+lbl&gt;, Superv &lt;dbl+lbl&gt;, EmpOCC &lt;dbl+lbl&gt;,\n#   TenureE &lt;dbl+lbl&gt;, SupParty &lt;dbl+lbl&gt;, ClosePty &lt;dbl+lbl&gt;,\n#   PARTYFW &lt;dbl+lbl&gt;, Idstrng &lt;dbl+lbl&gt;, RemLea &lt;dbl+lbl&gt;, RemLeaCl &lt;dbl+lbl&gt;,\n#   RemLeaSt &lt;dbl+lbl&gt;, Politics &lt;dbl+lbl&gt;, ConLabDf &lt;dbl+lbl&gt;, …\n\n\nThe above output is summarised in a haven- imported dataframe format also known as a ‘tibble’. For a really raw output we need to convert into a ‘pure’ data frame. Beware, the output might be very lengthy!\n\nhead(data.frame(bsa20))  \n\n     serial QnrVersion RespSx2cat RespAgeE MarStat6 REconFW01 REconFW02\n1 3.211e+09          1          2       70        5         0         0\n2 3.211e+09          1          2       66        1         0         0\n3 3.211e+09          1          1       64        1         0         0\n4 3.211e+09          1          2       43        1         0         0\n5 3.211e+09          1          1       38        1         0         0\n6 3.211e+09          1          2       77        1         0         0\n  REconFW03 REconFW04 REconFW05 REconFW06 REconFW07 REconFW08 REconFW09\n1         0         0         0         0         0         0         1\n2         0         0         0         0         0         0         1\n3         0         0         0         0         0         0         1\n4         1         0         0         0         0         0         0\n5         1         0         0         0         0         0         0\n6         0         0         0         0         0         0         1\n  REconFW10 REconFW11 EMPSTAT Employ Superv EmpOCC TenureE SupParty ClosePty\n1         0         0       1      2      1      3      10        1       NA\n2         0         0       1      2      1      1       1        1       NA\n3         0         0       1      1      2      1       1        1       NA\n4         0         0       1      3      1      3       1        2        2\n5         0         0       1      3      2      2       1        2        2\n6         0         0       3     NA     NA      1       9        1       NA\n  PARTYFW Idstrng RemLea RemLeaCl RemLeaSt Politics ConLabDf VoteDuty SocTrust\n1       1       2     NA       NA       NA        2       NA       NA        1\n2       2       3     NA       NA       NA        3       NA       NA        1\n3       2       3     NA       NA       NA        3       NA       NA        1\n4       2       3     NA       NA       NA        2       NA       NA        2\n5       1       3     NA       NA       NA        3       NA       NA        2\n6       1       2     NA       NA       NA        2       NA       NA        2\n  EngParl ScotPar2 ECPolicy2 Spend1 Spend2 SocBen1 SOCBEN2 DOLE TAXSPEND WkMent\n1      NA       NA        NA      2      1       1       2    1        2      1\n2      NA       NA        NA      1      3       2       5    1        2      2\n3      NA       NA        NA      3      1       2       3    1        2      2\n4      NA       NA        NA      7      3       1       2    2        2      2\n5      NA       NA        NA      7      3       2       4    2        2      1\n6      NA       NA        NA     98     NA       1       4    2        3      2\n  WkPhys HProbRsp PhsRetn PhsRecov MntRetn MntRecov HCWork21 HCWork22 HCWork23\n1      1        1       1        2       1        2        1        1        1\n2      2        1       1        3       1        2        1        0        1\n3      2        1       1        2       1        2        1        1        1\n4      2        2       2        3       1        2        1        1        1\n5      1        1       1        2       1        2        1        1        1\n6      2        2       2        2       2        2        1        0        1\n  HCWork24 HCWork25 HCWork26 HCWork28 HCWork29 HCWork213 HCWork214 HCWork215\n1        1        1        1        0        0         0         0         0\n2        1        1        1        0        0         0         0         0\n3        1        1        1        0        0         0         0         0\n4        1        1        1        0        0         0         0         0\n5        1        1        1        0        0         0         0         0\n6        1        1        0        0        0         0         0         0\n  HCWork27 CMtUnmar1 CMtUnmar2 CMtUnmar3 CMtUnmar4 CMtUnmar5 CMtUnmar6\n1        0         1         2         2         1         1         1\n2        0         1         1         1         3         3         1\n3        0         1         1         1         3         3         1\n4        0        NA        NA        NA        NA        NA        NA\n5        0        NA        NA        NA        NA        NA        NA\n6        0         1         1         1         3         1         8\n  CMtUnmar7 CMtUnmar8 CMtUnmar9 CMtUnmar10 CMtmar1 CMtmar2 CMtmar3 CMtmar4\n1         1         2         1          1      NA      NA      NA      NA\n2         1         1         3          1      NA      NA      NA      NA\n3         1         1         3          3      NA      NA      NA      NA\n4        NA        NA        NA         NA       1       1       2       1\n5        NA        NA        NA         NA       1       1       1       1\n6         1         1         3          1      NA      NA      NA      NA\n  CMtmar5 CMtmar6 CMtmar7 CMtmar8 CMtmar9 CMtmar10 ChCoSupp ChMIncM ChMIncF\n1      NA      NA      NA      NA      NA       NA        3       1      NA\n2      NA      NA      NA      NA      NA       NA        3       2      NA\n3      NA      NA      NA      NA      NA       NA        2       2      NA\n4       1       1       1       2       1        1       NA      NA       1\n5       1       1       1       2       1        1       NA      NA       1\n6      NA      NA      NA      NA      NA       NA        3       8      NA\n  ChMCont RBGaran2 RBGGov DigPCUn DigPCctl DigPCcon DigPCrsk DigGVun DigGVctl\n1       1        2     NA       2        2        2        1      NA       NA\n2       4        2     NA       2        3        3        1      NA       NA\n3       2        3     NA       3        3        3        8      NA       NA\n4      NA       NA     NA      NA       NA       NA       NA       1        2\n5      NA       NA     NA      NA       NA       NA       NA       3        3\n6       1        1      1       1        3        1        2      NA       NA\n  DigGVcon DigGVrsk DigPro NHSSat WkHmNow WkHmJan CovWkc CovNoWkc CovWkr1\n1       NA       NA      2      3      NA      NA     NA       NA      NA\n2       NA       NA      2      2      NA      NA     NA       NA      NA\n3       NA       NA      2      3      NA      NA     NA       NA      NA\n4        4        1      2      2       1       2     NA        1       0\n5        3        8      1      2       3       3      1       NA       0\n6       NA       NA      2      2      NA      NA     NA       NA      NA\n  CovWkr2 CovWkr3 CovWkr4 CovWkr5 CovWkr6 CovWk1 CovWk2 CovWk3 GovtWork\n1      NA      NA      NA      NA      NA     NA     NA     NA       NA\n2      NA      NA      NA      NA      NA     NA     NA     NA       NA\n3      NA      NA      NA      NA      NA     NA     NA     NA       NA\n4       0       0       0       1       0      5      5      5       NA\n5       0       0       0       0       1      3      3      3       NA\n6      NA      NA      NA      NA      NA     NA     NA     NA       NA\n  GovTrust CLRTRUST MPsTrust LoseTch VoteIntr PtyNMat2 PolPart01 PolPart02\n1       NA       NA       NA      NA       NA       NA        NA        NA\n2       NA       NA       NA      NA       NA       NA        NA        NA\n3       NA       NA       NA      NA       NA       NA        NA        NA\n4       NA       NA       NA      NA       NA       NA        NA        NA\n5       NA       NA       NA      NA       NA       NA        NA        NA\n6       NA       NA       NA      NA       NA       NA        NA        NA\n  PolPart03 PolPart04 PolPart05 PolPart06 PolPart07 PolPart08 PolPart09\n1        NA        NA        NA        NA        NA        NA        NA\n2        NA        NA        NA        NA        NA        NA        NA\n3        NA        NA        NA        NA        NA        NA        NA\n4        NA        NA        NA        NA        NA        NA        NA\n5        NA        NA        NA        NA        NA        NA        NA\n6        NA        NA        NA        NA        NA        NA        NA\n  PolPart10 PolPart11 REFHANG RefSyst UnempJob SocHelp DoleFidl WelfFeet\n1        NA        NA      NA      NA        3       4        4        4\n2        NA        NA      NA      NA        3       3        3        4\n3        NA        NA      NA      NA        3       4        4        4\n4        NA        NA      NA      NA        2       3        3        1\n5        NA        NA      NA      NA        2       4        2        3\n6        NA        NA      NA      NA        2       2        2        2\n  welfhelp morewelf damlives proudwlf Redistrb BigBusnN Wealth RichLaw Indust4\n1        4        2        2        1        3        4      3       5       4\n2        4        3        1        2        4        3      3       4       4\n3        3        3        1        1        3        3      2       3       3\n4        2        4        3        3        4        2      2       2       3\n5        3        3        3        2        4        2      3       3       4\n6        3        3        4        2        4        4      3       5       4\n  TradVals StifSent DeathApp Obey WrongLaw Censor NatIdGB ChAttend DisNew2\n1        3        3        2    3        4      3       5        7       2\n2        4        3        2    2        3      2       6       NA       2\n3        3        3        3    2        2      2       1       NA       2\n4        2        1        2    1        2      2       3        7       2\n5        4        3        3    3        4      2       3       NA       2\n6        1        2        3    1        3      2       3        1       2\n  DisAct HEdQual2 HhldEdu EURefV2 EUVOTWHO EURefb Voted Vote Anybn3 HHincome\n1     NA        2       2      NA       NA     NA     2   NA      1        2\n2     NA        1      NA      NA       NA     NA     1    2      2        3\n3     NA        2       1      NA       NA     NA     1    2      2        3\n4     NA        4       2      NA       NA     NA     1    1      1        4\n5     NA        3       2      NA       NA     NA     1    1      1        3\n6     NA        1      NA      NA       NA     NA     1    1      1        9\n  Maininc5 REarn HIncDif4 RetExp RetExpb FutrWrk PenKnow2 PenExp2 PenComp\n1        4    NA        3     NA      NA      NA       NA      NA      NA\n2        2    NA        2     NA      NA      NA       NA      NA      NA\n3        2    NA        2     NA      NA      NA       NA      NA      NA\n4        1     3        2      3      60       2        1    7000       4\n5        1     3        3      3      65       1        2     130       2\n6        1    NA        3     NA      NA      NA       NA      NA      NA\n  PenIntr INFORET3 WkPKnw WKPSav WkPSpn WPSvUs WPSvWw WPSvEas PrPKnw PrPSav\n1      NA       NA     NA     NA     NA     NA     NA      NA     NA     NA\n2      NA       NA     NA     NA     NA     NA     NA      NA     NA     NA\n3      NA       NA     NA     NA     NA     NA     NA      NA     NA     NA\n4       2        2      2      1      4      1      1       1     NA     NA\n5       2        2      3      1      4      1      2       2     NA     NA\n6      NA       NA     NA     NA     NA     NA     NA      NA     NA     NA\n  PrPSpn PrPSvUs PrPSvWW PrPSvEas NCOutcome Ragecat Ragecat20 DisActDV leftrigh\n1     NA      NA      NA       NA         1       7         6        3      3.8\n2     NA      NA      NA       NA         1       7         6        3      3.6\n3     NA      NA      NA       NA         1       6         5        3      2.8\n4     NA      NA      NA       NA         1       3         3        3      2.6\n5     NA      NA      NA       NA         1       3         3        3      3.2\n6     NA      NA      NA       NA         1       7         7        3      4.0\n   libauth welfare2 libauth2 leftrig2 welfgrp REconAct20 REconSum20 RaceOri4\n1 3.000000    2.000        2        3       1          9          6        3\n2 3.333333    2.375        2        3       1          9          6        3\n3 3.500000    2.125        2        2       1          9          6        3\n4 4.333333    3.625        3        2       3          3          2        3\n5 2.833333    3.000        2        2       2          3          2        3\n6 4.000000    3.500        3        3       2          9          6        3\n  LegMarStE HhlAdGpd HhlChlGpd BestNatU2 RetirAg3 ReligSum20 RlFamSum20\n1         4        1         0         1       65          3          1\n2         1        2         0         3       58          5          2\n3         1        2         0         1       54          5          1\n4         1        2         1         1       NA          3          2\n5         1        2         1         2       NA          5          3\n6         1        2         0         2       99          3          3\n  EmplStatDV RClassGP   serialh GOR gor2 BSA20_wt_new\n1          4        1 321100002   1    1    0.7099859\n2          6        1 321100014   1    1    0.3145871\n3          7        1 321100014   1    1    0.5649618\n4          4        1 321100040   1    1    0.9355446\n5          7        2 321100040   1    1    0.6830794\n6          3        1 321100042   1    1    1.4006989\n\n\nQuestions\n1. What is the overall sample size?\n2. How many variables are there in the dataset?\nNow, focus on the three variables we will use.\nNote Traditional statistical software such as SPSS or Stata treat categorical variables as arbitrary numbers. Values labels are then attached, that allocate a substantive meaning to these values. R on the other hand can either directly deal with the value themselves as alphanumeric variables, or with its own version of categorical variables, known as ‘factors’. There aren’t straightforward ways to convert SPSS or Stata labelled categorical variables into R factors.\nThe haven package that we use here preserves the original numeric values in the data, and add attributes that can be manipulated separately and contain the labels. Attributes are a special type of R objects that have a name, and can be read using the attr() function. Each variable has a ‘label’ and ‘labels’ attribute. The former is the variable description, the latter the value labels.\nAlternatively, haven-imported numeric variables can be converted into factors with levels (ie categories) reflecting the SPSS or Stata value labels, but with numeric values different from the original ones.\nLet’s examine the original variable description and value labels with the attr() function. We can do this variable by variable…\n\nattr(bsa20$TAXSPEND,\"label\")\n\n[1] \"If it had to choose, should govt reduce/increase/maintain levels of taxation and spending?\"\n\n\n… Or all at once:\n\nt(                          # Transpose rows and columns for better readability   \n  bsa20 |&gt; \n        select(TAXSPEND,EUVOTWHO,PenExp2) |&gt; # Select the relevant variables\n        summarise_all(attr,\"label\")  # Apply the attr() function to all of them\n)\n\n         [,1]                                                                                                 \nTAXSPEND \"If it had to choose, should govt reduce/increase/maintain levels of taxation and spending?\"         \nEUVOTWHO \"Did you vote to 'remain a member of the EU' or to 'leave the EU'?\"                                  \nPenExp2  \"How much do you think someone who reaches State Pension age today would receive in pounds per week?\"\n\n\nWe do the same with value labels:\n\nattr(bsa20$TAXSPEND,\"labels\")\n\n                                                        Not applicable \n                                                                    -1 \n  Reduce taxes and spend less on health, education and social benefits \n                                                                     1 \n    Keep taxes and spending on these services at the same level as now \n                                                                     2 \nIncrease taxes and spend more on health, education and social benefits \n                                                                     3 \n                                                            Don't know \n                                                                     8 \n                                                  Prefer not to answer \n                                                                     9 \n\nattr(bsa20$EUVOTWHO,\"labels\")\n\n                       Not applicable Remain a member of the European Union \n                                   -1                                     1 \n             Leave the European Union                      I Don't remember \n                                    2                                     3 \n                           Don't know                  Prefer not to answer \n                                    8                                     9 \n\n\nQuestion 3\nWhat do the variables measure and how?",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic population estimates with BSA data using R</span>"
    ]
  },
  {
    "objectID": "Population estimates using the BSAS with R.html#missing-values",
    "href": "Population estimates using the BSAS with R.html#missing-values",
    "title": "Basic population estimates with BSA data using R",
    "section": "2. Missing values",
    "text": "2. Missing values\nLet’s now examine the distribution of our three variables. We can temporarily convert EUVOTWHO and TAXSPEND into factors using mutate() for a more meaningful output that include their value labels. Review the frequency tables, examining the ‘not applicable’ and ‘don’t know’ categories.\n\nbsa20%&gt;%select(EUVOTWHO,TAXSPEND) %&gt;%\n       mutate(as_factor(.)) %&gt;%\n       summary()\n\n                                  EUVOTWHO   \n Not applicable                       :   0  \n Remain a member of the European Union: 635  \n Leave the European Union             : 463  \n I Don't remember                     :   2  \n Don't know                           :   0  \n Prefer not to answer                 :  21  \n NA's                                 :2843  \n                                                                   TAXSPEND   \n Not applicable                                                        :   0  \n Reduce taxes and spend less on health, education and social benefits  : 186  \n Keep taxes and spending on these services at the same level as now    :1589  \n Increase taxes and spend more on health, education and social benefits:2133  \n Don't know                                                            :  35  \n Prefer not to answer                                                  :  21  \n                                                                              \n\nsummary(bsa20$PenExp2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0     120     160    1293     200    9999    1076 \n\n\nQuestion 4\nWhy are there so many system missing values (NA) for EUVOTWHO and PenExp2 ? What does this mean when it comes to interpreting the percentages? You can use the documentation if needed.\nWhen analysing survey data, it is sometimes convenient to recode item nonresponses such as ´Don’t know´ and ‘Prefer not to say’ as system missing so that they do not appear in the results. An example of the syntax required to achieve this with EUVOTWHO and TAXSPEND is provided in the appendix.\nUnlike some other surveys, ‘Don’t knows’ and ‘Does not apply’ were not removed when weights were computed in the BSA. As a result, analyses using weights (ie when planning to use the data to make inference about the British population) need to retain these observations, otherwise estimated results might be incorrect.",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic population estimates with BSA data using R</span>"
    ]
  },
  {
    "objectID": "Population estimates using the BSAS with R.html#compare-unweighted-and-weighted-proportions",
    "href": "Population estimates using the BSAS with R.html#compare-unweighted-and-weighted-proportions",
    "title": "Basic population estimates with BSA data using R",
    "section": "3. Compare unweighted and weighted proportions",
    "text": "3. Compare unweighted and weighted proportions\nIn this section, we compare unweighted and weighted prportionso for EUVOTWHO and TAXSPEND. Let’s examine the unweighted responses first. In order to ensure coherence with the remainder of this exercise, we use xtabs() for categorical variables and summary() for continuous ones.\nFirst, as mentioned above, we recode EUVOTWHO and TAXSPENDinto factors, with value labels as levels using as_factor()\n\nbsa20&lt;-bsa20%&gt;%mutate(\n              TAXSPEND.f=as_factor(TAXSPEND,\"labels\"), \n              EUVOTWHO.f=as_factor(EUVOTWHO,\"labels\")\n               )\n\nWe can truncate factor levels respectively to 14 and 6 characters, for a more human-friendly output using substr():\n\nlevels(bsa20$TAXSPEND.f)&lt;-substr(levels(bsa20$TAXSPEND.f),1,14)\nlevels(bsa20$EUVOTWHO.f)&lt;-substr(levels(bsa20$EUVOTWHO.f),1,6)\n\nFinally, we compute the proportions:\n\nround(                               ### Rounds the results to one decimal\n  100*                               ### Converts proportions to %  \n    prop.table(                      ### Computes proportions\n      xtabs(~TAXSPEND.f,bsa20,       ### Computes frequencies,\n             drop.unused.levels = T) ### Leaves out levels with 0 observations),\n      ), \n  1)\n\nTAXSPEND.f\nReduce taxes a Keep taxes and Increase taxes     Don't know Prefer not to  \n           4.7           40.1           53.8            0.9            0.5 \n\nround(100*prop.table(xtabs(~EUVOTWHO.f,bsa20,drop.unused.levels = T)),1)\n\nEUVOTWHO.f\nRemain Leave  I Don' Prefer \n  56.6   41.3    0.2    1.9 \n\n\nWe can also examine the basic summary statistics for PenExp2:\n\nsummary(bsa20$PenExp2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0     120     160    1293     200    9999    1076 \n\n\nWhat is the (unweighted) percentage of respondents who say they voted remain in the EU referendum? About 57 percent of sample members who voted in referendum said they voted to remain. This figure seems a bit high (though people do not always report accurately).\nLet’s compare with the weighted frequencies. We will keep using xtabs() for convenience. With xtabs(), weights are specified on the left hand side of the formula as shown below. For the record, wtd.table() function from the Hmisc package also produces weighted frequency tables.\n\nxtabs(BSA20_wt_new~EUVOTWHO.f,\n      data=bsa20)\n\nEUVOTWHO.f\n    Not ap     Remain     Leave      I Don'     Don't      Prefer \n  0.000000 565.011079 489.146642   3.752765   0.000000  22.527320 \n\n\nWe can get rid of the empty levels to improve the output:\n\nxtabs(BSA20_wt_new~EUVOTWHO.f,\n      data=bsa20,\n      drop.unused.levels = T)\n\nEUVOTWHO.f\n    Remain     Leave      I Don'     Prefer \n565.011079 489.146642   3.752765  22.527320 \n\n\nWe convert the weighted frequencies into proportions and examine the results:\n\neuv.wp&lt;-round(\n  100*\n    prop.table(\n      xtabs(BSA20_wt_new~EUVOTWHO.f,\n            data=bsa20,\n            drop.unused.levels = T)\n      ),\n  1)\n\neuv.wp\n\nEUVOTWHO.f\nRemain Leave  I Don' Prefer \n  52.3   45.3    0.3    2.1 \n\n\nNow, what proportion say they voted remain in the EU referendum?\nIt is about 52 percent, lower than the unweighted proportion and closer to the actual referendum results.\nDo you have an idea as to why this might be the case?\nA possible explanation is that those more likely to vote ‘Remain’, such as younger people tend to also be less likely to take part in surveys, and therefore their real prevalence in the population will be underestimated by unweighted proportions.",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic population estimates with BSA data using R</span>"
    ]
  },
  {
    "objectID": "Population estimates using the BSAS with R.html#confidence-intervals",
    "href": "Population estimates using the BSAS with R.html#confidence-intervals",
    "title": "Basic population estimates with BSA data using R",
    "section": "4. Confidence intervals",
    "text": "4. Confidence intervals\nSo far, we have just computed point estimates without worrying about their precision. Estimates precision (or uncertainty) does matter insofar as it determines how big the ranges within which ‘true’ population values are likely to be. These are also known as the confidence intervals of our estimates.\nIn this exercise, we will be computing confidence intervals ’by hand‘ and ignore the survey design (ie whether clustering or stratification were used when collecting the sample) as the information is not available in this edition of the BSA. This amounts to assuming that the sample was collected using simple random sampling - which wasn’t the case - and increase the bias of our estimates.\nWe will explore the more reliable survey design functions provided by the survey package in the next exercise.\n\nConfidence intervals for proportions\nThe Hmisc package provides binconf() a handy function to compute confidence intervals for proportions. We need to provide it with two parameters: the frequencies for which we would like a confidence interval, and the total number of non missing observations. binconf() accepts individual proportions or complete frequency tables as input.\nWe begin with the unweighted confidence interval for EUVOTWHO:\n\neu.ci&lt;-binconf(xtabs(~EUVOTWHO.f,\n                     bsa20,\n                     drop.unused.levels = T)[1],\n               sum(xtabs(~EUVOTWHO.f,bsa20)))\n\neu.ci\n\n  PointEst     Lower     Upper\n 0.5664585 0.5372704 0.5951927\n\n\nWe convert the output into rounded percentages for better readability:\n\nround(100*\n      eu.ci,\n      1)\n\n PointEst Lower Upper\n     56.6  53.7  59.5\n\n\nWe can adapt the syntax above to make it work with weighted frequencies:\n\nround(100*\n  binconf(xtabs(bsa20$BSA20_wt_new~EUVOTWHO.f,\n                data=bsa20,\n                drop.unused.levels = T)[2],\n          sum(xtabs(bsa20$BSA20_wt_new~EUVOTWHO.f,\n                    data=bsa20,\n                    drop.unused.levels = T))),\n      1)\n\n PointEst Lower Upper\n     45.3  42.3  48.3\n\n\nWhat are the differences between weighted and unweighted confidence intervals for the proportion of people who voted remain?\nLet us now do the same with people’s views about government tax and spending.\n\nciprop&lt;-\nround(100*\nbinconf(xtabs(BSA20_wt_new~TAXSPEND.f,\n              data=bsa20,\n              drop.unused.levels=T),\n        sum(xtabs(BSA20_wt_new~TAXSPEND.f,\n                  bsa20))),\n1)\n\nciprop\n\n PointEst Lower Upper\n      5.5   4.8   6.3\n     42.8  41.3  44.3\n     50.3  48.8  51.9\n      0.9   0.6   1.2\n      0.5   0.3   0.8\n\n\nWe can improve the layout by adding the value labels. In order to do this, we create a data frame with the results of the above computation ciprop and specify that the row names should be the original value labels of TAXSPEND using as_factor. We also however need to omit the first label ‘Not applicable’ as we removed it earlier.\n\nciprop.l&lt;-data.frame(\n           ciprop,\n           row.names=levels(\n                     bsa20$TAXSPEND.f\n                     )[-1]\n           )\n\nciprop.l\n\n               PointEst Lower Upper\nReduce taxes a      5.5   4.8   6.3\nKeep taxes and     42.8  41.3  44.3\nIncrease taxes     50.3  48.8  51.9\nDon't know          0.9   0.6   1.2\nPrefer not to       0.5   0.3   0.8\n\n\nQuestion 5.\nWhat proportion think government should increase taxes and spend more on health, education and social benefits?\n\n\nConfidence intervals for means\nSeveral R packages offer functions for computing confidence intervals and standard errors of means. Here, we privilege doing things by hand in order to properly understand what is happening in the background.\nUnder assumptions of simple random sampling, a 95% confidence interval of the mean is defined as plus or minus 1.96 times its standard error. The standard error of the mean is its standard deviation – that is, the square root of its variance – divided by the square root of the sample size.\nWe will be using wtd.mean from the Hmisc package to compute weighted means, and wtd.var for variances. We can therefore compute:\n\nm.p&lt;-wtd.mean(bsa20$PenExp2,weights=bsa20$BSA20_wt_new)\nse.p&lt;-sqrt(wtd.var(bsa20$PenExp2,weights=bsa20$BSA20_wt_new))\nn&lt;-sum(bsa20$BSA20_wt_new[!is.na(bsa20$PenExp2)])\n\nci&lt;-c(m.p,m.p-1.96*(se.p/sqrt(n)),m.p+1.96*(se.p/sqrt(n)))\n\nround(ci,1)\n\n[1] 1305.9 1194.4 1417.5\n\n\nQuestion 6\nHow much do people think they will get at state pension age?",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic population estimates with BSA data using R</span>"
    ]
  },
  {
    "objectID": "Population estimates using the BSAS with R.html#answers",
    "href": "Population estimates using the BSAS with R.html#answers",
    "title": "Basic population estimates with BSA data using R",
    "section": "Answers",
    "text": "Answers\n\nThere are 3964 cases in the dataset.\nThe total number of variables is 212.\nTAXSPEND records responses to the questions of whether government should reduce/increase/maintain levels of taxation and spending. There are three possible responses to the question. EUVOTWHO records responses to the question ‘Did you vote to ’remain a member of the EU’ or to ‘leave the EU’?’ The responses are ‘Remain’ or ‘Leave’. *PenExp2 contains responses to the question ‘How much do you think someone who reaches State Pension age today would receive in pounds per week?’ Responses are numeric.\nThere are two reasons for the many ‘Not applicable’.\n\n\nRouting: the question is only asked to those who said yes to a previous question (EURefV2).\nVersions 5 and 6 - The BSA uses a split sample and the question is only asked in Versions 5 and 6.\n\n\nBetween 48.8 and 51.9% in the population say the government should increase taxes and spend more.\nThe amount people think they will get at state pension age varies between £1194 and £1417, with an average (ie mean) in the region of £1306.",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic population estimates with BSA data using R</span>"
    ]
  },
  {
    "objectID": "Population estimates using the BSAS with R.html#appendix-recoding-nonresponses-as-system-missing-na",
    "href": "Population estimates using the BSAS with R.html#appendix-recoding-nonresponses-as-system-missing-na",
    "title": "Basic population estimates with BSA data using R",
    "section": "Appendix: recoding nonresponses as system missing (NA)",
    "text": "Appendix: recoding nonresponses as system missing (NA)\nThe code below provides and example of how to recode missing values into system missing (NA) using separate variables. For ease of interpretation, we also convert the original numeric variable into labelled factors using as_factor(), so that they directly display the value labels.\n\nbsa20&lt;-bsa20%&gt;%mutate(\n              TAXSPEND.r=factor(as_factor(TAXSPEND,\"labels\"), \n                                exclude = c(\"Prefer not to answer\",\n                                            \"Don't know\")),\n              EUVOTWHO.r=factor(as_factor(EUVOTWHO,\"labels\"),\n                                exclude = c(\"Prefer not to answer\",\n                                            \"I Don't remember\",\n                                            \"Not applicable\",NA)),\n              PenExp2.r=ifelse(PenExp2==-1 | PenExp2&gt;=9998,NA,PenExp2)\n                      )\n### Value labels need to be truncated as they are rather lengthy!\nlevels(bsa20$TAXSPEND.r)&lt;-substr(levels(bsa20$TAXSPEND.r),1,14)\nlevels(bsa20$EUVOTWHO.r)&lt;-substr(levels(bsa20$EUVOTWHO.r),1,6)\n\nlevels(bsa20$TAXSPEND.r)\n\n[1] \"Reduce taxes a\" \"Keep taxes and\" \"Increase taxes\"\n\nlevels(bsa20$EUVOTWHO.r)\n\n[1] \"Remain\" \"Leave \"",
    "crumbs": [
      "R Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic population estimates with BSA data using R</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingSPSS.html",
    "href": "infer_w_survey_design_usingSPSS.html",
    "title": "Using SPSS with weights and survey design variables",
    "section": "",
    "text": "Getting started\nThis exercise is part of the ‘Introduction to the British Social Attitudes Survey (BSA)’ online module. In this exercise, we will practice statistical inference with data from the British Social Attitudes Survey (BSA) 2017 using weights and survey design variables.\nPlease note that at the time of writing this document only some of the BSA editions include survey design variables. For more information about inference from social surveys, including cases where weights and/or survey design variables are not available, please consult our guidelines.\nAnswers to the questions asked throughout the exercise can be found at the end of the page.\nData can be downloaded from the UK Data Service website following registration. Download the compressed folder, unzip and save it somewhere accessible on your computer.\nThe examples below assume that the dataset has been saved in a new folder named UKDS on your Desktop (Windows computers). The path would typically be C:\\Users\\YOUR_USER_NAME\\Desktop\\UKDS. Feel free to change it to the location that best suits your needs",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Using SPSS with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingSPSS.html#identifying-the-survey-design-and-variables",
    "href": "infer_w_survey_design_usingSPSS.html#identifying-the-survey-design-and-variables",
    "title": "Using SPSS with weights and survey design variables",
    "section": "1. Identifying the survey design and variables",
    "text": "1. Identifying the survey design and variables\nWe first need to find out about the survey design that was used in the BSA 2017, and the design variables available in the dataset. Such information can usually be found in the documentation that comes together with the data under the mrdoc/pdf folder or in the data catalogue pages for the data on the UK Data Service website.\nQuestion 1 What is the design that was used in this survey (ie how many stages were there, and what were the units sampled). What were the primary sampling units; the strata (if relevant)?\nNow that we are a bit more familiar with the way the survey was designed, we need to try and identify the design variables we can include when producing estimates. The information can usually be found in the user manual or the data dictionary available in the BSA documentation.\nQuestion 2 What survey design variables are available? Are there any that are missing – if so which ones? What is the name of the weights variables?",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Using SPSS with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingSPSS.html#specifying-the-survey-design",
    "href": "infer_w_survey_design_usingSPSS.html#specifying-the-survey-design",
    "title": "Using SPSS with weights and survey design variables",
    "section": "2. Specifying the survey design",
    "text": "2. Specifying the survey design\nLet us first open the 2017 BSA dataset.\nCD 'C:\\Users\\YOUR_USER_NAME\\Desktop\\UKDS'.\n  GET \n    FILE=' UKDA-8450-spss\\spss\\spss25\\bsa2017_for_ukda.sav'.\nIn principle, we should tell SPSS that we are working with a three stage stratified cluster sample. In practice however, we only have information about the initial ie primary sampling units.\nThis is achieved with the CSPLAN command through we create a plan file which contains the survey design information.\nCSPLAN ANALYSIS\n  /PLAN FILE='bsa17_SPSS_design.csaplan'\n  /PLANVARS ANALYSISWEIGHT=WtFactor       \n  /SRSESTIMATOR TYPE=WOR\n  /PRINT PLAN\n  /DESIGN STAGELABEL='S1' STRATA=StratID CLUSTER=Spoint \n  /ESTIMATOR TYPE=WR.",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Using SPSS with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingSPSS.html#mean-age-and-its-95-confidence-interval",
    "href": "infer_w_survey_design_usingSPSS.html#mean-age-and-its-95-confidence-interval",
    "title": "Using SPSS with weights and survey design variables",
    "section": "3. Mean age and its 95% confidence interval",
    "text": "3. Mean age and its 95% confidence interval\nWe can now produce a first set of estimates using this design and compare them with those we would have got without accounting for it. We will compute the average (ie mean) age of respondents in the sample, as well as the proportion of male and female respondents aged over 55. We will need to use /CSDESCRIPTIVES\nDATASET ACTIVATE DataSet1.\n* Complex Samples Descriptives.\nCSDESCRIPTIVES\n  /PLAN FILE='bsa17_SPSS_design.csaplan'\n  /SUMMARY VARIABLES=RAgeE\n  /MEAN\n  /STATISTICS SE CIN(95)\n  /MISSING SCOPE=ANALYSIS CLASSMISSING=EXCLUDE.\n\nUnder the /STATISTICS command we can request either or both the standard error of the mean and its 95% confidence interval.\nWhat difference would it make to the estimates and 95% CI to compute respectively, an unweighted mean, as well as a weighted mean without accounting for the survey design?\nUnweighted means and CI\nDESCRIPTIVES VARIABLES=RAgeE\n  /STATISTICS=MEAN SEMEAN.\nSPSS does not provide an option for computing confidence intervals in this case, but we know that a 95% confidence interval is the sample mean respectively minus and plus 1.96 times its standard error. Using the SPSS output, we can compute it ourselves as 1.96*.2872=about .56 years, that is close to 7 months.\nWeighted means and CI without survey design\nWEIGHT BY WtFactor.\nDESCRIPTIVES VARIABLES=RAgeE\n  /STATISTICS=MEAN SEMEAN.\nWEIGHT OFF.\nQuestion 3 What are the consequences of weighing but not accounting for the sample design; not using weights and accounting for the sample design when:\n\ninferring the mean value of the population age?\ninferring the uncertainty of our estimate of the population age?",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Using SPSS with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingSPSS.html#computing-a-proportion-and-its-95-confidence-interval",
    "href": "infer_w_survey_design_usingSPSS.html#computing-a-proportion-and-its-95-confidence-interval",
    "title": "Using SPSS with weights and survey design variables",
    "section": "4. Computing a proportion and its 95% confidence interval",
    "text": "4. Computing a proportion and its 95% confidence interval\nWe can now similarly compute an estimate of a proportion (or percentage) of a categorical variable in the population. For instance, the proportion of people who declare themselves interested in politics. This is the Politics variable. It has five categories that we are going to recode into ‘Significantly’ (interested) and ‘Not’ (significantly) in order to simplify the analysis.\nThe BSA regards ‘don’t know’ and ‘refusal’ responses as valid but since in this case there is only one ‘don’t know’ and no ‘refusal’, we can safely ignore these categories and recode them as system missing.\nFREQUENCIES VARIABLES=Politics\n  /ORDER=ANALYSIS.\n\nRECODE Politics (9=SYSMIS) (1 thru 2=1) (3 thru 5=2) INTO Politics.s.\nEXECUTE.\n\nVARIABLE LABELS\nPolitics.s   \"Whether significantly interested in politics\".\nVALUE LABELS\nPolitics.s\n1  \"Significant\"\n2  \"Not significant\". \nEXECUTE.\n\nFREQUENCIES VARIABLES=Politics.s\n  /ORDER=ANALYSIS.\n\nWEIGHT BY WtFactor.\nFREQUENCIES VARIABLES=Politics.s\n  /ORDER=ANALYSIS.\nWEIGHT OFF.\nAs with the mean of age earlier, we can see that the weighted and unweighted point estimates of the proportion of respondents significantly interested in politics change, even if slightly, and that they remain the same when survey design is accounted for.\nWith the help of CSTABULATE we can examine frequencies, proportions and confidence intervals of these proportions accounting for the survey design. As before, the point estimates do not further change once survey design is accounted for.\n* Complex Samples Frequencies.\nCSTABULATE\n  /PLAN FILE='bsa17_SPSS_design.csaplan'\n  /TABLES VARIABLES=Politics.s\n  /CELLS POPSIZE TABLEPCT\n  /STATISTICS CIN(95) \n  /MISSING SCOPE=TABLE CLASSMISSING=EXCLUDE.\nQuestion 4 What is the proportion of respondents aged 17-34 in the sample, as well as its 95% confidence interval? You can use RAgecat5",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Using SPSS with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingSPSS.html#domain-ie-subpopulation-estimates",
    "href": "infer_w_survey_design_usingSPSS.html#domain-ie-subpopulation-estimates",
    "title": "Using SPSS with weights and survey design variables",
    "section": "5. Domain (ie subpopulation) estimates",
    "text": "5. Domain (ie subpopulation) estimates\nAlthough computing estimates for specific groups (for example the average age of people who report being interested in politics) is not conceptually different from doing it for the sample as a whole, doing it with weights as part of an inferential analysis requires some caution. Calculating weighted estimates for a subpopulation while the rest of the sample is left out of the analysis might yield incorrect results. This is why using survey design informed functions is particularly recommended when doing such analyses.\nThe SPSS command CSDESCRIPTIVES that we used above makes such domain estimation relatively straightforward. If we would like to compute the mean age of BSA respondents by government office regions, we need to specify:\n\nThe outcome variable whose estimate we want to compute: ie RAgeE\nThe grouping variable(s) GOR_ID\nAnd the type of type of variance estimation we would like to see displayed ie standard errors or confidence interval\n\n * Complex Samples Descriptives.\nCSDESCRIPTIVES\n  /PLAN FILE='bsa17_SPSS_design.csaplan'\n  /SUMMARY VARIABLES=RAgeE\n  /SUBPOP TABLE=GOR_ID DISPLAY=LAYERED\n  /MEAN\n  /STATISTICS CIN(95)\n  /MISSING SCOPE=ANALYSIS CLASSMISSING=EXCLUDE.\n\nOur inference seem to suggest that the population in London is among the youngest in the country, and that those in the South West are among the oldest – their respective 95% confidence intervals do not overlap. We should not feel so confident about differences between London and the South East for example, as the CIs partially overlap.\nWe can also examine proportions for subpopulations. In order to do this, we need to specify the category of the variable we are interested in as an outcome.For instance, the syntax below uses respondents who are significantly interested in politics:\n* Complex Samples Frequencies.\nCSTABULATE\n  /PLAN FILE='bsa17_SPSS_design.csaplan'\n  /TABLES VARIABLES=Politics.s\n  /SUBPOP TABLE=GOR_ID DISPLAY=LAYERED\n  /CELLS TABLEPCT\n  /STATISTICS CIN(95) \n  /MISSING SCOPE=TABLE CLASSMISSING=EXCLUDE.\nQuestion 5 What is the 95% confidence interval for the proportion of people interested in politics in the South West? Is the proportion likely to be different in London? In what way? What is the region of the UK for which the precision of the estimates is likely to be the smallest?\nWhen using CSTABULATE, we can define domains or subpopulations with several variables, not just one. For example, we could look at gender differences in political affiliations by regions. However, as the size of subgroups decrease, so does the precision of the estimates as their confidence interval widens, to a point where their substantive interest is not meaningful anymore.\nQuestion 6 Using interest in politics as before, and three category age RAgecat5:\n\nProduce a table of results showing the proportion of respondents significantly interested in Politics by age group and gender\nAssess whether the age difference in interest for politics is similar for each gender?\nBased on the data, is it fair to say that men aged under 35 tend to be more likely to declare themselves interested in politics than women aged 55 and above?",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Using SPSS with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "infer_w_survey_design_usingSPSS.html#answers",
    "href": "infer_w_survey_design_usingSPSS.html#answers",
    "title": "Using SPSS with weights and survey design variables",
    "section": "Answers",
    "text": "Answers\nQuestion 1 The 2017 BSA is a three stage stratified random survey, with postcode sectors, adresses and individuals as the units selected at each stage. Primary sampling units were stratified according to geographies (sub regions), population density, and proportion of owner-occupiers. Sampling rate was proportional to the size of postcode sectors (ie number of addresses).\nQuestion 2 From the Data Dictionary it appears that the primary sampling units (sub regions) are identified by Spoint and the strata by StratID. The weights variable is WtFactor. Addresses are not provided but could be approximated with a household identifier.\nQuestion 3 Not using weights would make us overestimate the mean age in the population (of those aged 16+) by about 4 years. This is likely to be due to the fact that older respondents are more likely to take part to surveys. Using survey design variables does not alter the value of the estimated population mean. However, not accounting for it would lead us to overestimate the precision/underestimate the uncertainty of our estimate with a narrower confidence interval – by about plus or minus 3 months.\nQuestion 4 The proportion of 17-34 year old in the sample is 28.5 and its 95% confidence interval 26.5, 30.6\nQuestion 5 The 95% confidence interval for the proportion of people interested in politics in the South West is 39.8-53.4. By contrast, it is 47.6-60.8 in London. The region with the lowest precision of estimates (ie the widest confidence interval) is Wales, with a 20 percentage point difference between the upper and lower bounds of the confidence interval.\nQuestion 6\nCSTABULATE\n  /PLAN FILE='bsa17_SPSS_design.csaplan'\n  /TABLES VARIABLES=Politics.s\n  /SUBPOP TABLE=Rsex BY RAgecat5 DISPLAY=LAYERED\n  /CELLS TABLEPCT\n  /STATISTICS CIN(95) \n  /MISSING SCOPE=TABLE CLASSMISSING=EXCLUDE.\n\n\n\nSPSS output for POLITICS.s by Rsex and RAgecat5\n\n\nOlder respondents both male and female tend to be more involved in politics than younger ones.\nThe confidence intervals for the proportion of men under 35 and women above 55 interested in politics overlap; it is unlikely that they differ in the population.",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Using SPSS with weights and survey design variables</span>"
    ]
  },
  {
    "objectID": "Pop_estimates_using_the_BSAS_and_SPSS.html",
    "href": "Pop_estimates_using_the_BSAS_and_SPSS.html",
    "title": "Basic population estimates with BSA data using SPSS",
    "section": "",
    "text": "Getting started\nThis exercise is part of the ‘Introduction to the British Social Attitudes Survey (BSA)’ online module. In the exercise, we examine data from the 2020 British Social Attitudes survey to find out:\nAnswers to the questions asked throughout the exercise can be found at the end of the page.\nData can be downloaded from the UK Data Service website following registration. Download the compressed folder, unzip and save it somewhere accessible on your computer.\nThe examples below assume that the dataset has been saved in a new folder named UKDS on your Desktop (Windows computers). The path would typically be C:\\Users\\YOUR_USER_NAME\\Desktop\\UKDS. Feel free to change it to the location that best suits your needs.\nYou need to set the folder as your working directory in SPSS. To do this, you need to add the correct file path to the folder on your computer to the code below.\nIf you have your working directory saved to the folder location, the following code should open the BSA dataset.",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic population estimates with BSA data using SPSS</span>"
    ]
  },
  {
    "objectID": "Pop_estimates_using_the_BSAS_and_SPSS.html#getting-started",
    "href": "Pop_estimates_using_the_BSAS_and_SPSS.html#getting-started",
    "title": "Basic population estimates with BSA data using SPSS",
    "section": "",
    "text": "* Setting up the working directory\n* Change the command below  to match yours: \n\ncd \"C:\\Users\\YOUR_USER_NAME\\Desktop\\UKDS\".\nshow DIRECTORY.\n\n\n\nOutput of the show DIRECTORY command\n\n\n\nGET FILE='BSA\\UKDA-9005-spss\\spss\\spss25\\bsa2020_archive.sav'.\n\n\n\nBSA dataset in SPSS Variables View",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic population estimates with BSA data using SPSS</span>"
    ]
  },
  {
    "objectID": "Pop_estimates_using_the_BSAS_and_SPSS.html#explore-the-dataset",
    "href": "Pop_estimates_using_the_BSAS_and_SPSS.html#explore-the-dataset",
    "title": "Basic population estimates with BSA data using SPSS",
    "section": "1. Explore the dataset",
    "text": "1. Explore the dataset\nStart by getting an overall feel for the dataset. Either inspect variables and cases in the data editor or use the code below to produce a summary of all the variables in the dataset.\nCODEBOOK all. \n\n\n\nSPSS codebook output for the first variables\n\n\nQuestions\n1. What is the overall sample size? 2. How many variables are in the dataset?\nNow, focus on the three variables we will use.\nCODEBOOK TAXSPEND EUVOTWHO PenExp2.  \n\n\n\nSPSS codebook output for TAXSPEND\n\n\nQuestions 3\nWhat do the variables measure and how?",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic population estimates with BSA data using SPSS</span>"
    ]
  },
  {
    "objectID": "Pop_estimates_using_the_BSAS_and_SPSS.html#missing-values",
    "href": "Pop_estimates_using_the_BSAS_and_SPSS.html#missing-values",
    "title": "Basic population estimates with BSA data using SPSS",
    "section": "2. Missing values",
    "text": "2. Missing values\nReview the frequency tables, examining the not applicable and don’t know categories.\nQuestion 4\nWhy for EUVOTWHO are there so many not applicable? Note, you can use the documentation to check if needed. What does this mean when it comes to interpreting the percentages?\nWhen analysing survey data, it is sometimes convenient to recode item nonresponses such as ´Don’t know´ and ‘Prefer not to say’ as system missing so that they do not appear in the results. An example of the syntax required to achieve this with EUVOTWHO and TAXSPEND is provided in the appendix.\nUnlike some other surveys, ‘Don’t knows’ and ‘Does not apply’ were not removed when weights were computed in the BSA. As a result, analyses using weights (ie when planning to use the data to make inference about the British population) need to retain these observations, otherwise estimated results might be incorrect.",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic population estimates with BSA data using SPSS</span>"
    ]
  },
  {
    "objectID": "Pop_estimates_using_the_BSAS_and_SPSS.html#compare-unweighted-and-weighted-frequencies",
    "href": "Pop_estimates_using_the_BSAS_and_SPSS.html#compare-unweighted-and-weighted-frequencies",
    "title": "Basic population estimates with BSA data using SPSS",
    "section": "3. Compare unweighted and weighted frequencies",
    "text": "3. Compare unweighted and weighted frequencies\nLet’s examine the weighted responses.\nWEIGHT Off.\n*This line is probably not unnecessary as we have not applied a weight yet; it has been included just to make sure we are looking at unweighed results. \nFREQUENCIES VARIABLES=TAXSPEND EUVOTWHO\n  /BARCHART PERCENT\n  /ORDER=ANALYSIS.\n*Here, we use the FREQUENCIES command for the categorical variables and the EXAMINE command for the continous variables. \nEXAMINE VARIABLES=PenExp2\n  /PLOT HISTOGRAM\n  /STATISTICS DESCRIPTIVES\n  /MISSING LISTWISE\n  /NOTOTAL.\n\n\n\nSPSS output for Frequency distribution of TAXSPEND and EUVOTWHO\n\n\nWhat is the (unweighted) percent who say they voted remain in the EU referendum? The answer is about 58 percent of those who voted in the referendum say they voted to remain. This figure seems a bit high (though people do not always report accurately).\nLet’s add the weight.\n*The weight is added by the command below. It will remain on for all subsequent analyses. \nWEIGHT BY BSA20_wt_new.\nFREQUENCIES VARIABLES=TAXSPEND EUVOTWHO\n   /ORDER=ANALYSIS.\nEXAMINE VARIABLES=PenExp2\n  /PLOT HISTOGRAM\n  /STATISTICS DESCRIPTIVES\n  /CINTERVAL 95\n  /MISSING LISTWISE\n  /NOTOTAL.\n\n*To stop weighting the data you can use the following command. \nWEIGHT off. \n\n\n\nSPSS output for Frequency distribution of TAXSPEND and EUVOTWHO\n\n\nNow, what proportion say they voted remain in the EU referendum? It is about 54 percent, lower than the unweighted proportion and closer to the actual referendum results.",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic population estimates with BSA data using SPSS</span>"
    ]
  },
  {
    "objectID": "Pop_estimates_using_the_BSAS_and_SPSS.html#confidence-intervals",
    "href": "Pop_estimates_using_the_BSAS_and_SPSS.html#confidence-intervals",
    "title": "Basic population estimates with BSA data using SPSS",
    "section": "4. Confidence intervals",
    "text": "4. Confidence intervals\nAdd confidence intervals to the bar charts and mean to indicate uncertainty due to sampling error.\nWEIGHT BY BSA20_wt_new.\nGRAPH\n  /BAR(SIMPLE)=PCT BY TAXSPEND\n  /INTERVAL CI(95.0).\n\nGRAPH\n  /BAR(SIMPLE)=PCT BY EUVOTWHO\n  /INTERVAL CI(95.0).\n\nEXAMINE VARIABLES=PenExp2\n  /PLOT NONE\n  /STATISTICS DESCRIPTIVES\n  /CINTERVAL 95\n  /MISSING LISTWISE\n  /NOTOTAL.\n  \n\n\n\nSPSS output for GRAPH BAR of TAXSPEND and EUVOTWHO\n\n\nQuestion 5 \nWhat proportion think government should increase taxes and spend more on health, education and social benefits?\nQuestion 6 \nHow much do people think they will get at state pension age?\nAdditional question\nSelect two variables that interest you and examine their distribution.",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic population estimates with BSA data using SPSS</span>"
    ]
  },
  {
    "objectID": "Pop_estimates_using_the_BSAS_and_SPSS.html#answers",
    "href": "Pop_estimates_using_the_BSAS_and_SPSS.html#answers",
    "title": "Basic population estimates with BSA data using SPSS",
    "section": "Answers",
    "text": "Answers\n\nThere are 3964 cases in the dataset.\nThe total number of variables is 213.\nTAXSPEND records responses to the questions of whether government should reduce/increase/maintain levels of taxation and spending? There are three possible responses to the question. EUVOTWHO records responses to the question ‘Did you vote to ’remain a member of the EU’ or to ‘leave the EU’?’ The responses are Remain or Leave . PenExp2 contains responses to the question ’How much do you think someone who reaches State Pension age today would receive in pounds per week?’Responses are numeric.\nThere are two reasons for the many ‘not applicable’.\n\n\nRouting: the question is only asked to those who said yes to a previous question (EURefV2).\nVersions 5 and 6 - The BSA uses a split sample and the question is only asked in Versions 5 and 6.\n\n\nAbout 50% say the government should increase taxes and spend more.\nThe amount people think they will get at state pension age varies between £0 and £7000, with an average in the region between £170 and £184",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic population estimates with BSA data using SPSS</span>"
    ]
  },
  {
    "objectID": "Pop_estimates_using_the_BSAS_and_SPSS.html#appendix-recoding-nonresponses-as-system-missing",
    "href": "Pop_estimates_using_the_BSAS_and_SPSS.html#appendix-recoding-nonresponses-as-system-missing",
    "title": "Basic population estimates with BSA data using SPSS",
    "section": "Appendix: recoding nonresponses as system missing",
    "text": "Appendix: recoding nonresponses as system missing\nThe code below provides and example of how to recode missing values including ‘don’t knows’ and ‘prefer not to say’ into system missing.\nThe SPSS syntax below includes the command, the variables and the relevant missing values in (). Note, you can set missing values more than 1 at a time if they have the same missing value pattern.\nCOMPUTE EUVOTWHO_m=EUVOTWHO.\nCOMPUTE TAXSPEND_m=TAXSPEND.\nCOMPUTE PenExp2_m=PenExp2.\n\nMISSING VALUES PenExp2_m (-1, 9998, 9999). \nMISSING VALUES TAXSPEND_m (-1, 8, 9). \nMISSING VALUES EUVOTWHO_m (-1, 3 THRU 9).",
    "crumbs": [
      "SPSS Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic population estimates with BSA data using SPSS</span>"
    ]
  }
]