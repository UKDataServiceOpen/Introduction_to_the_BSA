---
title: "Basic population estimates with British Social Attitudes Survey data using R"
author: "UK Data Service"
date: last-modified
date-format: "MMMM YYYY"
mainfont: "Arial"
title-block-banner: "white"
title-block-banner-color: "#742082"
format:
#  docx: default
  html:
   toc: true
   smooth-scroll: true
   toc-location: left
   css: ukds.css
execute:
  warning: false
editor: 
  markdown: 
    wrap: sentence
---

This exercise is part of the ['Introduction to the British Social Attitudes Survey (BSA)'](https://trainingmodules.ukdataservice.ac.uk/attitudes/#/){target="_blank" rel="noopener"} online module.
In the exercise, we examine data from the 2020 British Social Attitudes survey to find out:

-   what proportion of respondents said they voted remain in the EU Referendum?

-   whether people think the government should raise taxes and spend more or reduce tax and cut social expenditures?

-   how much people think they'll get from the State pension?

Answers to the questions asked throughout the exercise can be found at the end of the page.

### Getting started

Data can be downloaded from the [UK Data Service website](https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=9005){target="_blank" rel="noopener"} following [registration](https://ukdataservice.ac.uk/help/registration/registration-login-faqs/){target="_blank" rel="noopener"}. Download the compressed folder, unzip and save it somewhere accessible on your computer.

The examples below assume that the dataset has been saved in a  new folder named *UKDS* on your Desktop (Windows computers). The path would typically be `C:\Users\YOUR_USER_NAME\Desktop\UKDS`. Feel free to change it to the location that best suits your needs. 

We begin by  loading  the R packages needed for the exercise and set the working directory.

```{r dummy}
#| eval: false
library(dplyr) ### Data manipulation functions
library(haven) ### Functions for importing data from 
               ### commercial packages
library(Hmisc) ### Extra statistical functions

### Setting up the working directory
### Please adjust the setwd() command  below   
### to match the location of the data on your computer 

setwd("C:\Users\Your_Username_here\")

getwd()
```

`
[1] C:\Users\Your_Username_here\
`

We then open the BSA dataset in SPSS format. Stata or tab-delimited format can also be used.

```{r}
#| eval: false
bsa20<-read_spss(
       'UKDA-9005-spss/spss/spss25/bsa2020_archive.sav'
       ) 
```



```{r files}
#| echo: false
#| output: false 
library(dplyr) ### Data manipulation functions
library(haven) ### Importing stata/SPSS files
library(Hmisc) ### Extra statistical functions

### Setting up the working directory
### Change the setwd() command  to match the location 
### of the data on your computer  if required 
### setwd(`C:\Users\Your_Username_here\`)

setwd("~/OneDrive/data/bsa")

getwd()

# Opening the BSA dataset in SPSS format
bsa20<-read_spss(
       'UKDA-9005-spss/spss/spss25/bsa2020_archive.sav'
       ) 
```

### 1. Explore the dataset

Start by getting an overall feel for the data.
Either inspect variables and cases in the data editor or use the code below to produce a summary of all the variables in the dataset.

```{r head1}
### Gives the number of rows (observations) 
### and columns (variables)
dim(bsa20) 

 ### List variable names in their actual 
 ### order in the dataset
names(bsa20)

### Displays the first five
### lines of a data frame

head(bsa20)  
```
The above output is summarised in a  `haven`- imported dataframe format also known as a 'tibble'. For a really raw output we need to convert into a 'pure' data frame. Beware, the output might be very lengthy!

```{r head2}
head(data.frame(bsa20))  
```

**Questions**

*1.  What is the overall sample size?*

*2.  How many variables are there in the dataset?*

Now, focus on the three variables we will use.

**Note** Traditional statistical software such as SPSS or Stata treat categorical variables as arbitrary numbers. Values labels are then attached, that allocate  a  substantive meaning to these values.
R on the other hand can either directly deal with the value themselves as alphanumeric variables, or with its own version of categorical variables, known as 'factors'.
There aren't straightforward ways to convert SPSS or Stata labelled categorical variables into R factors.

The  `haven` package that we use here  preserves the original numeric values in the data, and add attributes that can be manipulated separately and contain the labels. Attributes are a special type of R objects that have a name, and can be read using the `attr()` function. Each variable has a 'label' and 'labels' attribute. The former is the variable description, the latter the value labels.

Alternatively, haven-imported numeric variables can be converted into factors with levels (ie categories) reflecting the SPSS or Stata value labels, but with numeric values different from the original ones.

Let's examine the original variable description and value labels with the `attr()` function.
We can do this variable by variable...


```{r attr}
attr(bsa20$TAXSPEND,"label")
```

... Or all at once:

```{r attr1}
t(                          # Transpose rows and columns for better readability   
  bsa20 |> 
        select(TAXSPEND,EUVOTWHO,PenExp2) |> # Select the relevant variables
        summarise_all(attr,"label")  # Apply the attr() function to all of them
)
```

We do the same with value labels:


```{r attr2}

attr(bsa20$TAXSPEND,"labels")
attr(bsa20$EUVOTWHO,"labels")

```



**Question 3** 

*What do the variables measure and how?*

### 2. Missing values

Let's now examine the distribution of our three variables.
We can temporarily convert `EUVOTWHO` and `TAXSPEND` into factors using `mutate()` for a more meaningful output that include their value labels.
Review the frequency tables, examining the 'not applicable' and 'don't know' categories.

```{r summ}
bsa20%>%select(EUVOTWHO,TAXSPEND) %>%
       mutate(as_factor(.)) %>%
       summary()

summary(bsa20$PenExp2)
```


**Question 4** 

*Why are there so many system missing values (NA) for `EUVOTWHO` and `PenExp2` ?
What does this mean when it comes to interpreting the percentages? You can use the documentation if needed.*

When analysing survey data, it is sometimes convenient to recode item nonresponses such as ´Don’t know´ and ‘Prefer not to say’ as system missing so that they do not appear in the results. An example of the syntax required to achieve this with EUVOTWHO and TAXSPEND is provided in the appendix.


Unlike some other surveys,  'Don’t knows' and ‘Does not apply’ were not removed when weights were computed in the BSA. As a result,  analyses using weights (ie when planning to use the data to make inference about the British population) need to retain these observations, otherwise  estimated results might be incorrect.   
 
 

### 3. Compare unweighted and weighted proportions

In this section, we compare  unweighted and weighted prportionso for `EUVOTWHO` and  `TAXSPEND`. Let's examine the unweighted responses first.
In order to ensure coherence with the remainder of this exercise, we use `xtabs()` for categorical variables and `summary()` for  continuous ones.

First, as mentioned above, we recode `EUVOTWHO` and  `TAXSPEND`into factors, with value labels as levels using `as_factor()`
```{r factor}
bsa20<-bsa20%>%mutate(
              TAXSPEND.f=as_factor(TAXSPEND,"labels"), 
              EUVOTWHO.f=as_factor(EUVOTWHO,"labels")
               )
```


We can truncate factor levels respectively to 14 and 6 characters, for a more human-friendly output using `substr()`:

```{r factor1}
levels(bsa20$TAXSPEND.f)<-substr(levels(bsa20$TAXSPEND.f),1,14)
levels(bsa20$EUVOTWHO.f)<-substr(levels(bsa20$EUVOTWHO.f),1,6)
```

Finally, we compute the proportions:

```{r factor2}
round(                               ### Rounds the results to one decimal
  100*                               ### Converts proportions to %  
    prop.table(                      ### Computes proportions
      xtabs(~TAXSPEND.f,bsa20,       ### Computes frequencies,
             drop.unused.levels = T) ### Leaves out levels with 0 observations),
      ), 
  1)

round(100*prop.table(xtabs(~EUVOTWHO.f,bsa20,drop.unused.levels = T)),1)
```

We can also examine the basic summary statistics for `PenExp2`:

```{r pens}
summary(bsa20$PenExp2)
```



What is the (unweighted) percentage of respondents who say they voted remain in the EU referendum?
About `r round(100*prop.table(xtabs(~EUVOTWHO.f,bsa20)))[2]` percent of sample members who voted in referendum said they voted to remain.
This figure seems a bit high (though people do not always report accurately).

Let's compare with the weighted frequencies.
We will keep using `xtabs()` for convenience. With `xtabs()`, weights are specified on the left hand side of the formula as shown below. For the record,  `wtd.table()` function from the `Hmisc` package also produces weighted frequency tables.


```{r weight}
xtabs(BSA20_wt_new~EUVOTWHO.f,
      data=bsa20)
```
We can get rid of the empty levels to improve the  output:

```{r weight1}
xtabs(BSA20_wt_new~EUVOTWHO.f,
      data=bsa20,
      drop.unused.levels = T)
```


We convert the weighted frequencies  into proportions and examine the results:

```{r weight2}
euv.wp<-round(
  100*
    prop.table(
      xtabs(BSA20_wt_new~EUVOTWHO.f,
            data=bsa20,
            drop.unused.levels = T)
      ),
  1)

euv.wp
```


Now, what proportion say they voted remain in the EU referendum?

It is about `r round(100*prop.table(wtd.table(bsa20$EUVOTWHO.f,weights=bsa20$BSA20_wt_new)$sum.of.weights))[1]` percent, lower than the unweighted proportion and closer to the actual referendum results.

Do you have an idea as to why this might be the case?

A possible explanation is that those more likely to vote 'Remain', such as younger people tend to also be less likely to take part in surveys, and therefore their real prevalence in the population  will be underestimated by unweighted proportions. 

### 4. Confidence intervals

So far, we have just computed point estimates without worrying about their precision. Estimates precision (or uncertainty) does matter insofar as it determines how big the ranges within which  'true' population values are likely to be. These are also known as the *confidence intervals* of our estimates.

In this exercise, we will be computing confidence intervals ‘by hand‘ and ignore the survey design (ie whether clustering or stratification were used when collecting the sample) as the information is not available in this edition of the BSA. This amounts to assuming that the sample was collected using simple random sampling - which wasn’t the case - and increase the bias of our estimates. 

We will explore  the more reliable survey design functions provided by the `survey` package in the next exercise.

#### Confidence intervals for proportions

The `Hmisc` package provides `binconf()` a handy function to compute confidence intervals for proportions. We need to provide it with two parameters: the frequencies for which we would like a confidence interval, and the total number of non missing observations. `binconf()` accepts  individual proportions or  complete frequency tables as input. 

We begin with the unweighted confidence interval for EUVOTWHO: 

```{r ci}
eu.ci<-binconf(xtabs(~EUVOTWHO.f,
                     bsa20,
                     drop.unused.levels = T)[1],
               sum(xtabs(~EUVOTWHO.f,bsa20)))

eu.ci
```

We convert the output into rounded percentages for better readability:

```{r ci2}
round(100*
      eu.ci,
      1)
```

We can adapt the syntax above to make it work with weighted frequencies:

```{r ciprop1}
round(100*
  binconf(xtabs(bsa20$BSA20_wt_new~EUVOTWHO.f,
                data=bsa20,
                drop.unused.levels = T)[2],
          sum(xtabs(bsa20$BSA20_wt_new~EUVOTWHO.f,
                    data=bsa20,
                    drop.unused.levels = T))),
      1)
```

What are the differences between weighted and unweighted confidence intervals for the proportion of people who voted remain?

Let us now do the same with people's views about government tax and spending.

```{r ciprop2}
ciprop<-
round(100*
binconf(xtabs(BSA20_wt_new~TAXSPEND.f,
              data=bsa20,
              drop.unused.levels=T),
        sum(xtabs(BSA20_wt_new~TAXSPEND.f,
                  bsa20))),
1)

ciprop
```
We can improve the layout by adding the value labels.
In order to do this, we create a data frame with the results of the above computation `ciprop` and specify that the row names should be the original value labels of TAXSPEND using as_factor. We also however need to omit the first label 'Not applicable' as we removed it earlier.

```{r}
ciprop.l<-data.frame(
           ciprop,
           row.names=levels(
                     bsa20$TAXSPEND.f
                     )[-1]
           )

ciprop.l
```


**Question 5.** 

*What proportion think government should increase taxes and spend more on health, education and social benefits?*

#### Confidence intervals for means 


Several R packages offer functions for computing confidence intervals and standard errors of means.
Here, we privilege doing things by hand in order to properly understand what is happening in the background. 

Under assumptions of simple random sampling, a 95% confidence interval  of the mean is defined as plus or minus 1.96 times its  standard error. The standard error of the mean  is its  standard deviation -- that is, the square root of its variance -- divided by the square root of the sample size.

We will be using `wtd.mean` from the `Hmisc` package to compute weighted means, and `wtd.var` for variances.  We can therefore compute:

```{r cimean}
m.p<-wtd.mean(bsa20$PenExp2,weights=bsa20$BSA20_wt_new)
se.p<-sqrt(wtd.var(bsa20$PenExp2,weights=bsa20$BSA20_wt_new))
n<-sum(bsa20$BSA20_wt_new[!is.na(bsa20$PenExp2)])

ci<-c(m.p,m.p-1.96*(se.p/sqrt(n)),m.p+1.96*(se.p/sqrt(n)))

round(ci,1)
```

**Question 6** 

*How much do people think they will get at state pension age?*

### Answers

1.  There are `r nrow(bsa20)` cases in the dataset.

2.  The total number of variables is `r ncol(bsa20)`.

3.  *`TAXSPEND` records responses to the questions of whether government should reduce/increase/maintain levels of taxation and spending. There are three possible responses to the question.* `EUVOTWHO` records responses to the question 'Did you vote to 'remain a member of the EU' or to 'leave the EU'?'
    The responses are 'Remain' or 'Leave'.
    \*`PenExp2` contains responses to the question 'How much do you think someone who reaches State Pension age today would receive in pounds per week?'
    Responses are numeric.

4.  There are two reasons for the many 'Not applicable'.

-   Routing: the question is only asked to those who said yes to a previous question (EURefV2).
-   Versions 5 and 6 - The BSA uses a split sample and the question is only asked in Versions 5 and 6.

5.  Between `r ciprop[3,2]` and `r ciprop[3,3]`% in the population say the government should increase taxes and spend more.

6.  The amount people think they will get at state pension age varies between £`r round(ci[2])` and £`r round(ci[3])`, with an average (ie mean) in the region of £`r round(ci[1])`.


## Appendix: recoding nonresponses as system missing (NA)

The code below provides and example of how to recode missing values into system missing (NA) using separate variables. For ease of interpretation, we also convert the original numeric variable into labelled factors using `as_factor()`, so that they directly display the value labels.
```{r missing}
bsa20<-bsa20%>%mutate(
              TAXSPEND.r=factor(as_factor(TAXSPEND,"labels"), 
                                exclude = c("Prefer not to answer",
                                            "Don't know")),
              EUVOTWHO.r=factor(as_factor(EUVOTWHO,"labels"),
                                exclude = c("Prefer not to answer",
                                            "I Don't remember",
                                            "Not applicable",NA)),
              PenExp2.r=ifelse(PenExp2==-1 | PenExp2>=9998,NA,PenExp2)
                      )
### Value labels need to be truncated as they are rather lengthy!
levels(bsa20$TAXSPEND.r)<-substr(levels(bsa20$TAXSPEND.r),1,14)
levels(bsa20$EUVOTWHO.r)<-substr(levels(bsa20$EUVOTWHO.r),1,6)

levels(bsa20$TAXSPEND.r)
levels(bsa20$EUVOTWHO.r)

```
